{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.4.4 Seq2Seq con atención\n",
    "\n",
    "**RNN con arquitectura encoder-decoder y atención:**\n",
    "* **Sin atención** no va bien (en MT) porque el decoder no accede directamente a toda la entrada, sino que lo hace indirectamente a través de un \"cuello de botella\"\" (vector contextual)\n",
    "* **Con atención** usa un vector contextual dinámico (no fijo): $\\quad\\displaystyle\\boldsymbol{c}_t=\\sum_{i=1}^T\\alpha_i(\\boldsymbol{h}_{t-1}^d,\\boldsymbol{h}_{1:T}^e)\\,\\boldsymbol{h}_i^e$\n",
    "    * La query es el estado oculto del decoder en $t-1$, $\\boldsymbol{h}_{t-1}^d$\n",
    "    * Las claves son todos los estados ocultos del encoder, $\\boldsymbol{h}_{1:T}^e$\n",
    "    * Los valores también son los estados ocultos del encoder\n",
    "    * $\\boldsymbol{c}_t$ se concatena con el vector de entrada del decoder, $\\boldsymbol{y}_{t-1}$, y alimenta el decoder junto con $\\boldsymbol{h}_{t-1}^d$, para crear $\\boldsymbol{h}_t^d$\n",
    "\n",
    "**Ejemplo:** $\\;$ NMT de Inglés a Francés sin atención y con atención\n",
    "\n",
    "<div><table><tr>\n",
    "<td style=\"border: none;\"><img src=\"Figure_15.8_A.png\" width=\"550\"/></td>\n",
    "<td style=\"border: none;\"><img src=\"Figure_15.18.png\" width=\"450\"/></td>\n",
    "</tr></table></div>\n",
    "\n",
    "**Alineamientos (pesos de atención):** $\\;$ \"hace mucho frío aquí > it is very cold here\" y \"¿todavía están en casa? > are you still at home?\"\n",
    "\n",
    "<div><table align=center><tr>\n",
    "<td style=\"border: none;\"><img src=\"Figure_15.19_A.png\" width=\"400\"/></td>\n",
    "<td style=\"border: none;\"><img src=\"Figure_15.19_B.png\" width=\"450\"/></td>\n",
    "</tr></table></div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
