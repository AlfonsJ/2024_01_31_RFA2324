{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.7.3 EM para una GMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7.3.1 Paso E\n",
    "\n",
    "**Responsability:** $\\;$ del clúster $k$ en la generación del dato $n$, según la estimación actual de los parámetros $\\boldsymbol{\\theta}^{(t)}$,\n",
    "$$r_{nk}^{(t)}=p^*(z_n=k\\mid\\boldsymbol{y}_n,\\boldsymbol{\\theta}^{(t)})%\n",
    "=\\frac{\\pi_k^{(t)}\\,p(\\boldsymbol{y}_n\\mid\\boldsymbol{\\theta}_k^{(t)})}{\\sum_{k'}\\pi_{k'}^{(t)}\\,p(\\boldsymbol{y}_n\\mid\\boldsymbol{\\theta}_{k'}^{(t)})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7.3.2 Paso M\n",
    "\n",
    "**Log-verosimilitud completa esperada:** $\\;$ versión ponderada de la $\\operatorname{LL}$ para la Gaussiana multivariada; sea $z_{nk}=\\mathbb{I}(z_n=k)$,\n",
    "$$\\begin{align*}\n",
    "&\\operatorname{LL}^t(\\boldsymbol{\\theta})%\n",
    "=\\mathbb{E}\\left[\\sum_n\\log p(z_n\\mid\\boldsymbol{\\pi})+\\log p(\\boldsymbol{y}_n\\mid z_n,\\boldsymbol{\\theta})\\right]\\\\[5mm]%\n",
    "&=\\mathbb{E}\\left[\\sum_n\\log \\left(\\prod_k\\pi_k^{z_{nk}}\\right)+\\log\\left(\\prod_k%\n",
    "\\mathcal{N}(\\boldsymbol{y}_n\\mid\\boldsymbol{\\mu}_k,\\mathbf{\\Sigma}_k)^{z_{nk}}\\right)\\right]\\\\[5mm]%\n",
    "&=\\sum_n\\sum_k\\mathbb{E}[z_{nk}]\\log\\pi_k+\\sum_n\\sum_k\\mathbb{E}[z_{nk}]\\log%\n",
    "\\mathcal{N}(\\boldsymbol{y}_n\\mid\\boldsymbol{\\mu}_k,\\mathbf{\\Sigma}_k)\\\\[5mm]%\n",
    "&=\\sum_n\\sum_kr_{nk}^{(t)}\\log(\\pi_k)-\\frac{1}{2}\\sum_n\\sum_kr_{nk}^{(t)}\\,%\n",
    "[\\log\\lvert{\\mathbf{\\Sigma}_k}\\rvert+(\\boldsymbol{y}_n-\\boldsymbol{\\mu}_k)^t\\mathbf{\\Sigma}_k^{-1}(\\boldsymbol{y}_n-\\boldsymbol{\\mu}_k)]+\\operatorname{const}%\n",
    "\\end{align*}$$\n",
    "\n",
    "**Solución cerrada:** $\\;$ sea $r_k^{(t)}=\\sum_n r_{nk}^{(t)}$\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{\\mu}_k^{(t+1)}&=\\frac{1}{r_k^{(t)}}\\sum_nr_{nk}^{(t)}\\boldsymbol{y}_n\\\\%\n",
    "\\mathbf{\\Sigma}_k^{(t+1)}&=\\frac{1}{r_k^{(t)}}%\n",
    "\\sum_nr_{nk}^{(t)}(\\boldsymbol{y}_n-\\boldsymbol{\\mu}_k^{(t+1)})(\\boldsymbol{y}_n-\\boldsymbol{\\mu}_k^{(t+1)})^t%\n",
    "=\\frac{1}{r_k^{(t)}}\\left(\\sum_nr_{nk}^{(t)}\\boldsymbol{y}_n\\boldsymbol{y}_n^t\\right)%\n",
    "-\\boldsymbol{\\mu}_k^{(t+1)}(\\boldsymbol{\\mu}_k^{(t+1)})^t\\\\%\n",
    "\\pi_k^{(t+1)}&=\\frac{1}{N}\\sum_nr_{nk}^{(t)}=\\frac{r_k^{(t)}}{N}%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7.3.3 Ejemplo\n",
    "\n",
    "**Old Faithful:** $\\;$ GMM ajustado con el EM a datos 2d del géiser Old Faithful (minutos siguiente erupción vs duración; estandarizados)\n",
    "\n",
    "<center><img src=\"gmm_faithful.png\" width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7.3.4 Estimación MAP\n",
    "\n",
    "**Problema del colapso de la varianza:** $\\;$ si $\\mathbf{\\Sigma}_k=\\sigma_k^2\\mathbf{I}\\,$ y $\\,\\boldsymbol{\\mu}_k$ se asigna a un único punto, $\\boldsymbol{y}_n$, su verosilimilitud diverge con $\\sigma_k\\to 0$\n",
    "$$\\mathcal{N}(\\boldsymbol{y}_n\\mid\\boldsymbol{\\mu}_k=\\boldsymbol{y}_n,\\sigma^2_k\\mathbf{I})%\n",
    "=\\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\,e^0$$\n",
    "\n",
    "**Estimación MAP:** $\\;$ maximiza la log-verosimilitud completa esperada más un log-prior\n",
    "$$\\operatorname{LL}^t(\\boldsymbol{\\theta})=\\left[\\sum_n\\sum_kr_{nk}^{(t)}\\log\\pi_k+\\sum_n\\sum_kr_{nk}^{(t)}\\log p(\\boldsymbol{y}_n\\mid\\boldsymbol{\\theta}_k)\\right]+\\log p(\\boldsymbol{\\pi})+\\sum_k\\log p(\\boldsymbol{\\theta}_k)$$\n",
    "\n",
    "**Paso E:** $\\;$ igual que para el MLE\n",
    "\n",
    "**Paso M para los coeficientes:** $\\;$ con prior **Dirichlet,** $\\;\\boldsymbol{\\pi}\\sim\\operatorname{Dir}(\\boldsymbol{\\alpha})$, conjugada de la categórica,\n",
    "$$\\tilde{\\pi}_k^{(t+1)}=\\frac{r_k^{(t)}+\\alpha_k-1}{N+\\sum_k\\alpha_k-K}$$\n",
    "* Coincide con el MLE con un prior uniforme, $\\alpha_k=1$\n",
    "\n",
    "**Paso M para las componentes:** $\\;$ con prior **Normal-Inverse-Wishart,** $\\;$ conjugada de la Gaussiana multivariada,\n",
    "$$p(\\boldsymbol{\\mu}_k,\\mathbf{\\Sigma}_k)%\n",
    "=\\operatorname{NIW}(\\boldsymbol{\\mu}_k,\\mathbf{\\Sigma}_k\\mid\\overset{\\smile}{\\boldsymbol{m}},\\overset{\\smile}{\\kappa},\\overset{\\smile}{\\nu},\\overset{\\smile}{\\mathbf{S}})$$\n",
    "\n",
    "* Con $\\overset{\\smile}{\\kappa}=0$, las $\\boldsymbol{\\mu}_k$ no se regularizan, por lo que el prior solo afecta a las $\\mathbf{\\Sigma}_k$ y los estimadores MAP son:\n",
    "$$\\begin{align*}\n",
    "\\tilde{\\boldsymbol{\\mu}}_k^{(t+1)}&=\\hat{\\boldsymbol{\\mu}}_k^{(t+1)}\\\\%\n",
    "\\tilde{\\mathbf{\\Sigma}}_k^{(t+1)}&=\\frac{\\overset{\\smile}{\\mathbf{S}}+\\hat{\\mathbf{\\Sigma}}_k^{(t+1)}}{\\overset{\\smile}{\\nu}+r_k^{(t)}+D+2}%\n",
    "\\end{align*}$$\n",
    "\n",
    "* **Covarianza a priori:** $\\;$ si $s_d=\\frac{1}{N}\\sum_n(x_{nd}-\\bar{x}_d)^2$ es la varianza global en la dimensión $d$, una posibilidad consiste en usar\n",
    "$$\\overset{\\smile}{\\mathbf{S}}=\\frac{1}{K^{1/D}}\\operatorname{diag}(s_1^2,\\dotsc,s_D^2)$$\n",
    "\n",
    "* El hiperparámetro $\\overset{\\smile}{\\nu}$ controla la fuerza del prior; una elección usual es el prior propio más débil: $\\;\\overset{\\smile}{\\nu}=D+1$\n",
    "\n",
    "**Ejemplo:** $\\;$ mixtura de $K=2$ componentes a ajustar con $N=100$ datos sintéticos en $D$ dimensiones ($D=1$ en la gráfica)\n",
    "\n",
    "<div><table border-collapse: collapse><tr>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top; padding:0; margin:0;\" width=500>\n",
    "\n",
    "<center><img src=\"gmm_singularity.png\" width=400></center>\n",
    "\n",
    "La primera componente es un pico estrecho (con $\\sigma_1\\approx 0$) centrado en un único dato $x_1$\n",
    "\n",
    "</td>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top; padding:0; margin:0;\" width=500>\n",
    "\n",
    "<center><img src=\"gmm_mle_vs_map.png\" width=400></center>\n",
    "\n",
    "Fracción del número de veces (de $5$ intentos) que el EM presenta problemas numéricos con MLE y MAP\n",
    "\n",
    "</td></tr></table></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7.3.5 No-convexidad de la NLL\n",
    "\n",
    "**No-convexidad de la NLL:** $\\;$ la log-verosimilitud de una mixtura suele tener múltiples modas, esto es, más de un óptimo global\n",
    "$$\\operatorname{LL}(\\boldsymbol{\\theta})=\\sum_{n=1}^N\\log\\sum_{z_n=1}^Kp(\\boldsymbol{y}_n,z_n\\mid\\boldsymbol{\\theta})$$\n",
    "\n",
    "**Ejemplo:** $\\;200$ puntos de una mixtura de $2$ Gaussianas 1d con $\\pi_k=0.5$, $\\sigma_k=5$, $\\mu_1=-10$ y $\\mu_2=10$; y $\\,\\operatorname{LL}(\\mu_1,\\mu_2)\\,$ bimodal\n",
    "\n",
    "<div><table border-collapse: collapse><tr>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top; padding:0; margin:0;\" width=500>\n",
    "\n",
    "<center><img src=\"gmm_lik_surface_hist.png\" width=500></center>\n",
    "\n",
    "</td>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top; padding:0; margin:0;\" width=500>\n",
    "\n",
    "<center><img src=\"gmm_lik_surface_contour.png\" width=500></center>\n",
    "\n",
    "</td></tr></table></div>\n",
    "\n",
    "**Label switching problem:** $\\;$ número de modas exponencial con $K$ ($K!$ potenciales), por lo que únicamente podemos aspirar a encontrar un buen óptimo local con una buena inicialización"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
