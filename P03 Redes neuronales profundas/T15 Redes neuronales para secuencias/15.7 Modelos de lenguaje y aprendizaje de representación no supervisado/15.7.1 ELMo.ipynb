{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.7.1 ELMo\n",
    "\n",
    "**Embeddings from Language Model (ELMo):** $\\;$ ajusta dos modelos de lenguaje RNN, uno izqda-dcha y otro dcha-izqda, y combina sus estados ocultos para formar un embedding por palabra\n",
    "\n",
    "**Objetivo:** $\\;$ a diferencia del entrenamiento de una LSTM-BLSTM, con pares entrada-salida, ELMo se entrena sin supervisión, para minimizar la log-verosimilitud negativa de la frase de entrada\n",
    "$$\\mathcal{L}(\\boldsymbol{\\theta})=-\\sum_{t=1}^T%\n",
    "[\\log p(x_t|\\boldsymbol{x}_{1:t-1};\\boldsymbol{\\theta}_e,\\boldsymbol{\\theta}^{\\to},\\boldsymbol{\\theta}_s)%\n",
    "+\\log p(x_t|\\boldsymbol{x}_{t+1:T};\\boldsymbol{\\theta}_e,\\boldsymbol{\\theta}^{\\gets},\\boldsymbol{\\theta}_s)]$$\n",
    "con parámetros compartidos de la capa de embedding y de la softmax, $\\boldsymbol{\\theta}_e$ y $\\boldsymbol{\\theta}_s$, así como parámetros propios de ambos modelos $\\boldsymbol{\\theta}^{\\to}$ y $\\boldsymbol{\\theta}^{\\gets}$\n",
    "\n",
    "**Modelo bidireccional ELMo:** $\\;y_t=x_{t+1}$ en modo LSTM hacia adelante y $\\,y_t=x_{t-1}\\,$ en modo LSTM hacia atrás\n",
    "<div align=center><img src=\"Figure_15.32.png\" width=600></div>\n",
    "\n",
    "**Representación contextual tras entrenamiento:** $\\;\\boldsymbol{r}_t=[\\boldsymbol{e}_t,\\boldsymbol{h}_{t,1:L}^\\to,\\boldsymbol{h}_{t,1:L}^\\gets]\\,$ con una LSTM de $L$ capas\n",
    "\n",
    "**Fine-tuning:** $\\;$ se aprende un conjunto lineal de pesos dependiente de la tarea para transformar $\\boldsymbol{r}_t$ en el embedding final de cada token\n",
    "* **Part-of-speech (POS) tagging:** $\\;$ el modelo aprenderá a dar más peso a las capas inferiores\n",
    "* **Word sense disambiguation (WSD):** $\\;$ el modelo aprenderá a dar más peso a las capas superiores\n",
    "* **Datos de adaptación:** $\\;$ tanto en POS tagging como en WSD se necesita un conjunto pequeño de datos etiquetados específicos de la tarea con el que aprender un único vector de pesos para transformar $\\boldsymbol{r}_{1:T}$ en las etiquetas objetivo $\\boldsymbol{y}_{1:T}$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
