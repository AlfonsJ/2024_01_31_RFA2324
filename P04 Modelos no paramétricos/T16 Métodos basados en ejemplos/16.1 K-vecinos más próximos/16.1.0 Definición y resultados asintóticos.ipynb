{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición y resultados asintóticos\n",
    "\n",
    "Dada una entrada $\\boldsymbol{x}$, el clasificador (por los) **$K$ vecinos más próximos** o **K nearest neighbor (KNN)** busca $K$ ejemplos o prototipos más cercanos de $\\boldsymbol{x}$ en el conjunto de entrenamiento, $N_K(\\boldsymbol{x},\\mathcal{D})$, y toma sus etiquetas para derivar una distribución sobre las salidas en la región local alrededor de $\\boldsymbol{x}$:\n",
    "$$p(y=c\\mid\\boldsymbol{x},\\mathcal{D})%\n",
    "=\\frac{1}{K}\\sum_{n\\in N_K(\\boldsymbol{x},\\mathcal{D})}\\mathbb{I}(y_n=c)$$\n",
    "Más simplemente, KNN retorna la etiqueta más votada (mayoritaria) si es única; si no, en caso de empate a votos entre dos o más clases, KNN devuelve la etiqueta del prototipo más cercano entre los prototipos de las clases empatadas.\n",
    "\n",
    "El caso particular $K=1$ es especialmente popular; se conoce como clasificador (por el) **vecino más próximo** o **nearest neighbor (NN)** y su función predictiva es una delta:\n",
    "$$p(y=c\\mid\\boldsymbol{x},\\mathcal{D})=\\delta(c,y_n)%\n",
    "\\quad\\text{con conjunto unitario}\\quad%\n",
    "N_1(\\boldsymbol{x},\\mathcal{D})=\\{n\\}$$\n",
    "Conviene destacar que podría no existir un único prototipo más cercano a $\\boldsymbol{x}$, sino dos o más empatados a la misma distancia de $\\boldsymbol{x}$. En tal caso asumimos que el desempate se decide al azar. Más generalmente, en el caso de KNN, si existen dos o más conjuntos de $K$ prototipos más cercanos a $\\boldsymbol{x}$, escogemos uno de ellos al azar. Nótese que el empate a distancias no tiene nada que ver con el empate a votos. Por lo general, la probabilidad de que se produzcan empates a distancia es insignificante, no así la probabilidad de empate a votos; por eso desempatamos al azar en el caso de empates a distancia, pero no en el caso de empate a votos.\n",
    "\n",
    "Los parámetros principales de KNN son el tamaño del entorno local, $K$, y la distancia $d(\\boldsymbol{x},\\boldsymbol{x}')$ con la que compara cualquier par de puntos en el espacio de representación de los datos, típicamente $\\mathbb{R}^D$. Se suele usar la distancia Euclídea o, más generalmente, la de Mahalanobis:\n",
    "$$d_M(\\boldsymbol{x},\\boldsymbol{\\mu})=\\sqrt{(\\boldsymbol{x}-\\boldsymbol{\\mu})^tM(\\boldsymbol{x}-\\boldsymbol{\\mu})}$$\n",
    "donde $M$ es una matriz definida positiva; por ejemplo, si $M$ es la identidad $M=I_D$, Mahalanobis coincide con la Euclídea.\n",
    "\n",
    "En términos asíntóticos, esto es, cuando $N\\to\\infty$, el clasificador NN comete un error de clasificación no superior a dos veces el de Bayes. Más aún, el error del clasificador KNN converge al de Bayes si $K$ se escoge tal que $K\\to\\infty$ y $K/N\\to 0$; esto se cumple, por ejemplo, tomando $K=\\sqrt{N}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
