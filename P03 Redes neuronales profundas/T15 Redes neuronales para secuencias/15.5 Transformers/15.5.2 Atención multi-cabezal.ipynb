{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.5.2 Atención multi-cabezal\n",
    "\n",
    "**Multi-headed attention (MHA):** $\\;$ introduce $H\\geq 1$ matrices (kernels) de atención para capturar diferentes nociones de similitud\n",
    "\n",
    "Dada una consulta $\\boldsymbol{q}\\in\\mathbb{R}^{d_q}$, claves $\\boldsymbol{k}_j\\in\\mathbb{R}^{d_k}$ y valores $\\boldsymbol{v}_j\\in\\mathbb{R}^{d_v}$, el $i$-ésimo cabezal de atención es\n",
    "$$\\boldsymbol{h}_i=\\operatorname{Attn}(\\mathbf{W}_i^{(q)}\\boldsymbol{q},\\{\\mathbf{W}_i^{(k)}\\boldsymbol{k}_j,\\mathbf{W}_i^{(v)}\\boldsymbol{v}_j\\})\\in\\mathbb{R}^{p_v}%\n",
    "\\qquad\\text{donde}\\quad%\n",
    "\\mathbf{W}_i^{(q)}\\in\\mathbb{R}^{p_q\\times d_q},\\;%\n",
    "\\mathbf{W}_i^{(k)}\\in\\mathbb{R}^{p_k\\times d_k},\\;\\text{y}\\;%\n",
    "\\mathbf{W}_i^{(v)}\\in\\mathbb{R}^{p_v\\times d_v}$$\n",
    "\n",
    "Los $H$ cabezales se apilan y proyectan en $\\mathbb{R}^{p_o}$:\n",
    "$$\\boldsymbol{h}=\\operatorname{MHA}(\\boldsymbol{q},\\{\\boldsymbol{k}_j,\\boldsymbol{v}_j\\})=\\mathbf{W}_o(\\boldsymbol{h}_1,\\dotsc,\\boldsymbol{h}_h)^t\\in\\mathbb{R}^{p_o}\n",
    "\\qquad\\text{donde}\\quad%\n",
    "\\mathbf{W}_o\\in\\mathbb{R}^{p_o\\times Hp_v}$$\n",
    "\n",
    "Representación:\n",
    "<div align=center><img src=\"Figure_15.24.png\" width=600></td></tr></table></div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
