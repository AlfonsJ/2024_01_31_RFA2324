{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.3.3.4 Todo junto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente de la p√©rdida respecto a los logits $\\boldsymbol{a}$\n",
    "$$\\boldsymbol{u}_2=\\boldsymbol{\\nabla}_{\\boldsymbol{a}}\\mathcal{L}%\n",
    "=\\biggl(\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{a}}\\biggr)^t%\n",
    "=(\\boldsymbol{p}-\\boldsymbol{y})^t\\in\\mathbb{R}^{1\\times C}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u2 = tf.Tensor([[-0.2689  0.2689] [ 0.2689 -0.2689] [ 0.2689 -0.2689] [-0.7311  0.7311]], shape=(4, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "u2 = y_pred - y\n",
    "print('u2 =', str(u2).replace('\\n',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respecto a la linealidad de la capa de salida $\\mathbf{V}$\n",
    "$$\\boldsymbol{\\nabla}_{\\mathbf{V}}\\mathcal{L}%\n",
    "=\\biggl[\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{V}}\\biggr]_{1,:}%\n",
    "=\\biggl[\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{a}}\\frac{\\partial\\boldsymbol{a}}{\\partial\\mathbf{V}}\\biggr]_{1,:}%\n",
    "=\\biggl[\\boldsymbol{u}_2^t\\frac{\\partial\\boldsymbol{a}}{\\partial\\mathbf{V}}\\biggr]_{1,:}%\n",
    "=\\boldsymbol{u}_2\\boldsymbol{h}^t\\in\\mathbb{R}^{C\\times K}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.2689  0.2689], shape=(2,), dtype=float32) tf.Tensor([0.  0.5], shape=(2,), dtype=float32) tf.Tensor(\n",
      "[[-0.     -0.1345]\n",
      " [ 0.      0.1345]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(u2[0], h[0, :], tf.tensordot(u2[0], h[0, :], axes=0))\n",
    "#gV = u2[0] @ tf.transpose(h[0])\n",
    "#print(u2[0], tf.transpose(h[0]), gV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "\\boldsymbol{\\nabla}_{\\mathbf{V}}\\mathcal{L}%\n",
    "&=\\biggl[\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{V}}\\biggr]_{1,:}%\n",
    "=\\boldsymbol{u}_2\\boldsymbol{h}^t\\in\\mathbb{R}^{C\\times K}\\\\%\n",
    "\\boldsymbol{\\nabla}_{\\boldsymbol{b}_2}\\mathcal{L}%\n",
    "&=\\biggl(\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{b}_2}\\biggr)^t%\n",
    "=\\boldsymbol{u}_2\\in\\mathbb{R}^C\\\\%\n",
    "\\boldsymbol{u}_1=\\boldsymbol{\\nabla}_{\\boldsymbol{z}}\\mathcal{L}%\n",
    "&=\\biggl(\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{z}}\\biggr)^t%\n",
    "=(\\mathbf{V}^t\\boldsymbol{u}_2)\\odot H(\\boldsymbol{z})\\in\\mathbb{R}^K\\\\%\n",
    "\\boldsymbol{\\nabla}_{\\mathbf{W}}\\mathcal{L}%\n",
    "&=\\biggl[\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}}\\biggr]_{1,:}%\n",
    "=\\boldsymbol{u}_1\\boldsymbol{x}^t\\in\\mathbb{R}^{K\\times D}\\\\%\n",
    "\\boldsymbol{\\nabla}_{\\boldsymbol{b}_1}\\mathcal{L}%\n",
    "&=\\biggl(\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{b}_1}\\biggr)^t%\n",
    "=\\boldsymbol{u}_1\\in\\mathbb{R}^K\\\\%\n",
    "\\boldsymbol{\\nabla}_{\\boldsymbol{x}}\\mathcal{L}%\n",
    "&=\\biggl(\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{x}}\\biggr)^t%\n",
    "=\\mathbf{W}^t\\boldsymbol{u}_1\\in\\mathbb{R}^D%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z = L1_preact(X); print('z =', str(z).replace('\\n',''))\n",
    "h = L1(X); print('h =', str(h).replace('\\n',''))\n",
    "L2logits = L2_preact(h); print('logits =', str(L2logits).replace('\\n','')) # softmax\n",
    "y_pred = L2(h); print('y_pred =', str(y_pred).replace('\\n',''))\n",
    "eL2 = np.exp(L2logits)\n",
    "y_pred_alt = np.transpose(eL2.T / eL2.sum(axis=1))\n",
    "print('y_pred_alt =', str(y_pred_alt).replace('\\n','')) # softmax a mano\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "print('loss =', loss(y, y_pred)); \n",
    "loss_alt_all = -y * np.log(y_pred) # loss a mano\n",
    "print('loss_alt_data =', str(loss_alt_all).replace('\\n',''))\n",
    "print('loss_alt =', tf.reduce_sum(loss_alt_all) / 4.) # SUM_OVER_BATCH_SIZE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
