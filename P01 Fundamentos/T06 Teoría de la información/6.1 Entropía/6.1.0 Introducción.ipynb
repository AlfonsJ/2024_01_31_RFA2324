{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "La **entropía** de una distribución de probabilidad es una medida de incertidumbre, o falta de predecibilidad, asociada con una variable aleatoria extraída de dicha distribución. En teoría de la información se usa para definir el contenido de información de una fuente de datos. Por ejemplo, sea una secuencia de símbolos $X_n\\sim p$ generados por una distribución $p$. Si $p$ tiene alta entropía, es difícil predecir el valor de cada $X_n$ y decimos que $\\mathcal{D}=\\{X_1,\\dotsc,X_n\\}$ tiene alto contenido informativo. Por el contrario, si $p$ es una distribución degenerada con entropía mínima (nula), todos los $X_n$ son idénticos y $\\mathcal{D}$ no contiene mucha información."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
