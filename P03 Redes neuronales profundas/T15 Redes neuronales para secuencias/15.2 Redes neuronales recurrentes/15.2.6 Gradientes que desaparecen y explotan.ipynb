{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.2.6 Gradientes que desaparecen y explotan\n",
    "\n",
    "**Vanishing and exploding gradients:** $\\;$ las activaciones en una RNN tienden a desaparecer o explotar...\n",
    "* en forward, ya que multiplicamos la matriz $\\mathbf{W}_{hh}$ en cada $t$\n",
    "* y backward, ya que multiplicamos Jacobianas en cada $t$\n",
    "\n",
    "**Gradient clipping:** $\\;$ solución sencilla para los que explotan\n",
    "\n",
    "**Reservoir computing:** $\\;$ control del radio espectral $\\lambda$ del mapping forward, $\\mathbf{W}_{hh}$, y del backward, dado por la Jacobiana $\\mathbf{J}_{hh}$\n",
    "* **Echo state network (ESN):** $\\;$ inicializa aleatoriamente $\\mathbf{W}_{hh}$ tal que $\\lambda\\approx 1$ y la mantiene fija, por lo que solo aprende $\\mathbf{W}_{oh}$\n",
    "* **Liquid state machine (LSM):** $\\;$ ESN com neuronas binarias\n",
    "\n",
    "**Otra posibilidad:** $\\;$ optimización restringida de $\\mathbf{W}_{hh}$ para mantenerla ortogonal\n",
    "\n",
    "**Alternativa:** $\\;$ modificar la arquitectura de la RNN con actualizaciones aditivas de los estados ocultos, en lugar de multiplicativas"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
