{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.4.4 Seq2Seq con atención\n",
    "\n",
    "**RNN con arquitectura encoder-decoder sin atención:** $\\;$ no da buen resultado en MT ya que el decoder no puede acceder directamente a las palabras de entrada, sino que lo hace indirectamente a través de un \"cuello de botella\"\" (vector contextual)\n",
    "\n",
    "**RNN con arquitectura encoder-decoder y atención:**\n",
    "<div><table><tr>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top;\" width=500>\n",
    "\n",
    "Usa un vector contextual dinámico (no fijo):\n",
    "$$\\boldsymbol{c}_t=\\sum_{i=1}^T\\alpha_i(\\boldsymbol{h}_{t-1}^d,\\boldsymbol{h}_{1:T}^e)\\,\\boldsymbol{h}_i^e$$\n",
    "\n",
    "* La query es el estado oculto del decoder en $t-1$, $\\boldsymbol{h}_{t-1}^d$\n",
    "* Las claves son todos los estados ocultos del encoder, $\\boldsymbol{h}_{1:T}^e$\n",
    "* Los valores también son los estados ocultos del encoder\n",
    "* $\\boldsymbol{c}_t$ se concatena con el vector de entrada del decoder, $\\boldsymbol{y}_{t-1}$, y alimenta el decoder junto con $\\boldsymbol{h}_{t-1}^d$, para crear $\\boldsymbol{h}_t^d$\n",
    "\n",
    "</td><td style=\"border: none;\"><img src=\"Figure_15.18.png\" width=\"500\"/></td>\n",
    "</tr></table></div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
