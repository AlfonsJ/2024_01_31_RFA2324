{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.3.9 Ejercicios de regresión logística multinomial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1:** Sea un problema de clasificación en $C=3$ clases, y sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\operatorname{Cat}(y\\mid\\mathcal{S}(\\mathbf{W}^t\\boldsymbol{x}+\\boldsymbol{b}))$, un modelo de regresión logística multinomial con\n",
    "$$\\mathbf{W}^t=\\begin{pmatrix}1&1\\\\-1&1\\\\0&0\\end{pmatrix}\\in\\mathbb{R}^{C\\times D}%\n",
    "\\qquad\\text{y}\\qquad%\n",
    "\\boldsymbol{b}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$$\n",
    "Determina la probabilidad de que $\\boldsymbol{x}=[0.5; 0.5]$ pertenezca a la clase $1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\mathcal{S}([1, 1; -1, 1; 0, 0][0.5; 0.5])_1%\n",
    "=\\mathcal{S}([1; 0; 0])_1=\\dfrac{e}{e+2}=\\dfrac{1}{1+2/e}=0.5761$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2:** $\\quad$ Sea $\\mu_c=\\mathcal{S}(\\boldsymbol{\\eta})_c$ con logits $\\boldsymbol{\\eta}\\in\\mathbb{R}^C$. Prueba que\n",
    "$\\;\\dfrac{\\partial\\mu_c}{\\partial\\eta_j}=\\mu_c(\\delta_{cj}-\\mu_j)\\;$ donde $\\;\\delta_{cj}=\\mathbb{I}(c=j)$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad$ Sea $S=\\sum_ie^{\\eta_i}$ el denominador de la softmax.\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial\\mu_c}{\\partial\\eta_j}%\n",
    "&=\\frac{\\partial}{\\partial\\eta_j}\\,\\left[e^{\\eta_c}S^{-1}\\right]\\\\[2mm]%\n",
    "&=\\left[\\frac{\\partial}{\\partial\\eta_j}\\,e^{\\eta_c}\\right]S^{-1}%\n",
    "+e^{\\eta_c}(-S^{-2})\\frac{\\partial}{\\partial\\eta_j}S\\\\[2mm]%\n",
    "&=(\\delta_{cj}\\,e^{\\eta_c})S^{-1}-e^{\\eta_c}e^{\\eta_j}S^{-2}\\\\[2mm]%\n",
    "&=\\frac{e^{\\eta_c}}{S}\\left(\\delta_{cj}-\\frac{e^{\\eta_j}}{S}\\right)\\\\[2mm]%\n",
    "&=\\mu_c(\\delta_{cj}-\\mu_j)%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3:** $\\;$ Sea un problema de clasificación en $C$ clases, y sea $p(y\\mid\\boldsymbol{x};\\mathbf{W})=\\operatorname{Cat}(y\\mid \\mathcal{S}(\\mathbf{W}^t\\boldsymbol{x}))$, $y\\in\\{1,2,\\dotsc,C\\}$, $\\mathbf{W}\\in\\mathbb{R}^{D\\times C}$, un modelo de regresión logística multinomial en notación compacta (homogénea). Sea el conjunto de entrenamiento $\\mathcal{D}=\\{(\\boldsymbol{x}_n,\\boldsymbol{y}_n)\\}$, donde $\\boldsymbol{x}_n\\in\\mathbb{R}^D$ y $\\boldsymbol{y}_n\\in\\{0,1\\}^C$, $\\sum_cy_{nc}=1$. \n",
    "La neg-log-verosimilitud de $\\mathbf{W}$ es:\n",
    "$$\\operatorname{NLL}(\\mathbf{W})=-\\frac{1}{N}\\sum_n\\sum_cy_{nc}\\log\\mu_{nc}$$\n",
    "donde $\\boldsymbol{\\mu}_n=\\mathcal{S}(\\boldsymbol{a}_n)$ con logits $\\boldsymbol{a}_n=\\mathbf{W}^t\\boldsymbol{x}_n$. Demuestra que el gradiente de la neg-log-verosimilitud con respecto a $\\mathbf{W}^t$ es:\n",
    "$$\\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}^t}=\\frac{1}{N}\\sum_{n=1}^N\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad$ Sea $\\boldsymbol{w}_j$ el vector (columna) de pesos asociados con la clase $j$\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial\\operatorname{NLL}}{\\partial{\\boldsymbol{w}_j}}%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n\\sum\\nolimits_c%\n",
    "\\frac{y_{nc}}{\\mu_{nc}}\\frac{\\partial\\mu_{nc}}{\\partial \\boldsymbol{w}_j}\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n\\sum\\nolimits_c%\n",
    "\\frac{y_{nc}}{\\mu_{nc}}\\frac{\\partial\\mu_{nc}}{\\partial\\eta_{nj}}\n",
    "\\frac{\\partial\\eta_{nj}}{\\partial \\boldsymbol{w}_j}\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n\\sum\\nolimits_c%\n",
    "\\frac{y_{nc}}{\\mu_{nc}}\\mu_{nc}(\\delta_{cj}-\\mu_{nj})\\boldsymbol{x}_n\\\\%\n",
    "&=\\frac{1}{N}\\sum\\nolimits_n\\sum\\nolimits_c y_{nc}(\\mu_{nj}-\\delta_{cj})\\boldsymbol{x}_n\\\\%\n",
    "&=\\frac{1}{N}\\sum\\nolimits_n\\left(%\n",
    "\\left(\\sum\\nolimits_cy_{nc}\\right)\\mu_{nj}\\boldsymbol{x}_n%\n",
    "-\\sum\\nolimits_c\\delta_{cj}y_{nc}\\boldsymbol{x}_n\\right)\\\\%\n",
    "&=\\frac{1}{N}\\sum\\nolimits_n(\\mu_{nj}-y_{nj})\\boldsymbol{x}_n%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 4:** Sea un problema de clasificación en $C=3$ clases, y sea $p(y\\mid\\boldsymbol{x};\\mathbf{W})=\\operatorname{Cat}(y\\mid \\mathcal{S}(\\mathbf{W}^t\\boldsymbol{x}))$, $y\\in\\{1,2,\\dotsc,C\\}$, $\\mathbf{W}\\in\\mathbb{R}^{D\\times C}$, un modelo de regresión logística multinomial en notación compacta (homogénea). Sea el conjunto de entrenamiento $\\mathcal{D}=\\{(\\boldsymbol{x}_n,\\boldsymbol{y}_n)\\}$, donde $\\boldsymbol{x}_n\\in\\mathbb{R}^D$ y $\\boldsymbol{y}_n\\in\\{0,1\\}^C$, $\\sum_cy_{nc}=1$. Sabemos que el gradiente de la neg-log-verosimilitud con respecto a $\\mathbf{W}$ es:\n",
    "$$\\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}}=\\frac{1}{N}\\sum_{n=1}^N\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t$$\n",
    "donde $\\boldsymbol{\\mu}_n=\\mathcal{S}(\\boldsymbol{a}_n)$ con logits $\\boldsymbol{a}_n=\\mathbf{W}^t\\boldsymbol{x}_n$. Supón que estamos aplicando descenso por gradiente estocástico con minibatch de talla $1$ para minimizar la $\\operatorname{NLL}$. Más concretamente, nos hallamos en la iteración $i$ con $\\mathbf{W}_i=[1, 0, 1; -1, 1, -1; 0, 0, 1]$, $\\eta_i=0.1$ y el minibatch actual únicamente incluye la muestra $\\boldsymbol{x}_n=(1, 1, 1)^t$, de clase $y_n=1$. Determina $\\mathbf{W}_{i+1}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{a}_n%\n",
    "&=\\mathbf{W}_t^t\\boldsymbol{x}_n%\n",
    "=\\begin{pmatrix}1&-1&0\\\\0&1&0\\\\1&-1&1\\end{pmatrix}\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}%\n",
    "=\\begin{pmatrix}0\\\\1\\\\1\\end{pmatrix}\\\\[3mm]%\n",
    "\\boldsymbol{\\mu}_n%\n",
    "&=S(\\boldsymbol{a}_n)%\n",
    "=\\dfrac{1}{1+2e}\\begin{pmatrix}1\\\\e\\\\e\\end{pmatrix}%\n",
    "=\\begin{pmatrix}0.1554\\\\0.4223\\\\0.4223\\end{pmatrix}\\\\[3mm]%\n",
    "\\mathbf{W}_{t+1}%\n",
    "&=\\mathbf{W}_t-\\eta_t\\,\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t\\\\%\n",
    "&=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}-0.1\\,\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}(-0.8446, 0.4223, 0.4223)\\\\%\n",
    "&=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}-\\begin{pmatrix}-0.0845&-0.0845&-0.0845\\\\0.0422&0.0422&0.0422\\\\0.0422&0.0422&0.0422\\end{pmatrix}\\\\%\n",
    "&=\\begin{pmatrix}1.0845&0.0845&1.0845\\\\-1.0422&0.9578&-1.0422\\\\-0.0422&-0.0422&0.9578\\end{pmatrix}\n",
    "\\end{align*}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
