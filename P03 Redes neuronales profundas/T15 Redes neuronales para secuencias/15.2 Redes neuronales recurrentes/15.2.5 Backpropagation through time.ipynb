{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.2.5 Backpropagation through time\n",
    "\n",
    "**Backpropagation through time (BPTT):** $\\;$ backprop aplicado al grafo computacional de una RNN **desenrrollado (unrolled)** en el tiempo con el fin de hallar un MLE de sus parámetros\n",
    "$$\\mathbf{\\Theta}^*=\\operatorname*{argmax}_{\\mathbf{\\Theta}}\\;p(\\boldsymbol{y}_{1:T}\\mid\\boldsymbol{x}_{1:T},\\mathbf{\\Theta})$$\n",
    "\n",
    "**Modelo y pérdida:**\n",
    "<div style=\"text-align:left;\">\n",
    "<table style=\"text-align:left;\"><tr>\n",
    "<td style=\"border:none; text-align:left; vertical-align:top;\" width=450>\n",
    "\n",
    "Modelo con logits de salida $\\boldsymbol{o}_t$ y sin sesgos por simplicidad:\n",
    "$$\\begin{align*}\n",
    "  \\boldsymbol{h}_t&=\\mathbf{W}_{hx}\\boldsymbol{x}_t+\\mathbf{W}_{hh}\\boldsymbol{h}_{t-1}\\\\%\n",
    "  \\boldsymbol{o}_t&=\\mathbf{W}_{ho}\\boldsymbol{h}_t%\n",
    "\\end{align*}$$\n",
    "\n",
    "Pérdida con etiquetas de salida $y_t$:\n",
    "$$L=\\frac{1}{T}\\sum_{t=1}^T\\ell(y_t,\\boldsymbol{o}_t)$$\n",
    "\n",
    "</td><td style=\"border:none;\" width=50></td>\n",
    "<td style=\"border: none;\"><img src=\"Figure_15.9.png\" width=550/></td>\n",
    "</tr></table>\n",
    "</div>\n",
    "\n",
    "**Cálculo de gradientes:** $\\displaystyle\\quad\\frac{\\partial L}{\\partial\\mathbf{W}_{hx}},\\quad\\frac{\\partial L}{\\partial\\mathbf{W}_{hh}},\\quad\\frac{\\partial L}{\\partial\\mathbf{W}_{ho}}$\n",
    "* El último es fácil de calcular ya que es local a cada $t$\n",
    "* Los dos primeros dependen del estado oculto, por lo que requieren mirar atrás en el tiempo...\n",
    "* Con $\\mathbf{W}_h$ igual a $\\mathbf{W}_{hh}$ y $\\mathbf{W}_{hx}$ aplanadas y apiladas\n",
    "$$\\begin{align*}\n",
    "  \\boldsymbol{h}_t&=f(\\boldsymbol{x}_t,\\boldsymbol{h}_{t-1},\\mathbf{W}_h)\\\\%\n",
    "  \\boldsymbol{o}_t&=g(\\boldsymbol{h}_t,\\mathbf{W}_o)%\n",
    "\\end{align*}$$\n",
    "* Por la regla de la cadena:\n",
    "$$\\frac{\\partial L}{\\partial\\mathbf{W}_h}%\n",
    "  =\\frac{1}{T}\\sum_{t=1}^T\\frac{\\partial\\ell(y_y,\\boldsymbol{o}_t)}{\\partial\\mathbf{W}_h}%\n",
    "  =\\frac{1}{T}\\sum_{t=1}^T%\n",
    "  \\frac{\\partial\\ell(y_y,\\boldsymbol{o}_t)}{\\partial\\boldsymbol{o}_t}%\n",
    "  \\frac{\\partial g(\\boldsymbol{h}_t,\\mathbf{W}_o)}{\\partial\\boldsymbol{h}_t}%\n",
    "  \\frac{\\partial\\boldsymbol{h}_t}{\\partial\\mathbf{W}_h}$$\n",
    "* El último término puede expandirse como sigue:\n",
    "$$\\frac{\\partial\\boldsymbol{h}_t}{\\partial\\mathbf{W}_h}%\n",
    "  =\\frac{\\partial f(\\boldsymbol{x}_t,\\boldsymbol{h}_{t-1},\\mathbf{W}_h)}{\\partial\\mathbf{W}_h}%\n",
    "  +\\frac{\\partial f(\\boldsymbol{x}_t,\\boldsymbol{h}_{t-1},\\mathbf{W}_h)}{\\partial\\boldsymbol{h}_{t-1}}%\n",
    "  \\frac{\\partial\\boldsymbol{h}_{t-1}}{\\partial\\mathbf{W}_h}$$\n",
    "* Si se sigue expandiendo recursivamente, se llega a:\n",
    "$$\\frac{\\partial\\boldsymbol{h}_t}{\\partial\\mathbf{W}_h}%\n",
    "  \\!=\\!\\frac{\\partial f(\\boldsymbol{x}_t,\\boldsymbol{h}_{t-1},\\mathbf{W}_h)}{\\partial\\mathbf{W}_h}%\n",
    "  +\\!\\sum_{i=1}^{t-1}\\!\\left[\\prod_{j=i+1}^t%\n",
    "    \\frac{\\partial f(\\boldsymbol{x}_j,\\boldsymbol{h}_{j-1},\\mathbf{W}_h)}{\\partial\\boldsymbol{h}_{j-1}}\\right]\\!%\n",
    "  \\frac{\\partial f(\\boldsymbol{x}_i,\\boldsymbol{h}_{i-1},\\mathbf{W}_h)}{\\partial\\mathbf{W}_h}$$\n",
    "* **Coste temporal:** $\\;O(T)$ por paso; $O(T^2)$ en total\n",
    "* **Reducción del coste:** $\\;$ suma de $K$ términos más recientes\n",
    "* **Aplicación a secuencias largas:** $\\;$ con ventanas no solapadas sucesivas, manteniendo el estado oculto de la RNN en batches sucesivos durante el entrenamiento (sin barajar subsecuencias)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
