{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.4.3 Atención paramétrica\n",
    "\n",
    "**Atención aditiva:** $\\;$ dada una query $\\boldsymbol{q}\\in\\mathbb{R}^q$ y una clave $\\boldsymbol{k}\\in\\mathbb{R}^k$, la función de scoring mediante atención aditiva es\n",
    "$$a(\\boldsymbol{q},\\boldsymbol{k})%\n",
    "=\\boldsymbol{w}_v^t\\tanh(\\mathbf{W}_q\\boldsymbol{q}+\\mathbf{W}_k\\boldsymbol{k})\\in\\mathbb{R}%\n",
    "\\quad\\text{con}\\quad%\n",
    "\\mathbf{W}_q\\in\\mathbb{R}^{h\\times q},\\quad%\n",
    "\\mathbf{W}_k\\in\\mathbb{R}^{h\\times k}\\quad\\text{y}\\quad%\n",
    "\\mathbf{W}_v\\in\\mathbb{R}^h$$\n",
    "* Equivale a un MLP con $\\boldsymbol{q}\\in\\mathbb{R}^q$ y $\\boldsymbol{k}\\in\\mathbb{R}^k$ concatenados de entrada y una capa oculta de $h$ unidades sin sesgos\n",
    "\n",
    "**Atención producto escalar escalado:** $\\;$ si $\\,\\boldsymbol{q},\\boldsymbol{k}\\in\\mathbb{R}^d\\,$ exhiben media nula y varianza identidad, su producto escalar escalado por $1/\\sqrt{d}$ también exhibe media nula y varianza unitaria\n",
    "$$a(\\boldsymbol{q},\\boldsymbol{k})=\\boldsymbol{q}^t\\boldsymbol{k}/\\sqrt{d}\\in\\mathbb{R}$$\n",
    "\n",
    "* Con minibatches de talla $n$: $\\;\\mathbf{Q}\\in\\mathbb{R}^{n\\times d}$, $\\mathbf{K}\\in\\mathbb{R}^{m\\times d}$, $\\mathbf{V}\\in\\mathbb{R}^{m\\times v}$\n",
    "$$\\operatorname{Attn}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\mathcal{S}\\bigl(\\mathbf{Q}\\mathbf{K}^t/\\sqrt{d}\\bigr)\\mathbf{V}\\in\\mathbb{R}^{n\\times v}\\qquad\\text{(softmax aplicada por filas)}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
