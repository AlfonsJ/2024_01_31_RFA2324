{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.5.5 Redes neuronales Bayesianas\n",
    "\n",
    "**Entrenamiento convencional de DNNs:** $\\;$ máxima verosimilitud (penalizada) para hallar un único ajuste de parámetros; no obstante, los modelos usuales tienen muchos más parámetros que puntos de datos, por lo que es de esperar que haya múltiples modelos que, ajustándose igual de bien que el entrenado, generalicen de formas diferentes\n",
    "\n",
    "**Bayesian neural network (BNN):** $\\;$ una manera de capturar la incertidumbre inducida consiste en marginalizar parámetros mediante la **distribución predictiva a posteriori**\n",
    "$$p(\\boldsymbol{y}\\mid\\boldsymbol{x},\\mathcal{D})%\n",
    "=\\int p(\\boldsymbol{y}\\mid\\boldsymbol{x},\\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta}\\mid\\mathcal{D})\\,d\\boldsymbol{\\theta}$$\n",
    "\n",
    "Este resultado puede interpretarse como un ensamble infinito de redes ponderadas diferentemente; la marginalización de parámetros permite evitar el sobre-entrenamiento, si bien es muy costosa"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
