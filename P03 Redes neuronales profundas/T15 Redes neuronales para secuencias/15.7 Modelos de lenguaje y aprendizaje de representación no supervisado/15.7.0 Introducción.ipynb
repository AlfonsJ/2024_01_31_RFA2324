{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.7.0 Introducción\n",
    "\n",
    "**Contextual word embeddings:**\n",
    "* Las RNNs y transformers auto-regresivos (solo-decoder) son modelos de lenguaje que generan secuencias de la forma:\n",
    "$$p(x_1,\\dotsc,x_T)=\\prod_{t=1}^Tp(x_t\\mid\\boldsymbol{x}_{1:t-1})$$\n",
    "* En lugar de un vector one-hot $\\boldsymbol{x}_t$ o un embedding aprendido, usamos los estados latentes de dichos modelos a modo de embedding contextual para tareas seq2seq o clasificación de texto\n",
    "\n",
    "**Transfer learning:**\n",
    "* **Pre-training:** $\\;$ pre-entrenamos un modelo de lenguaje sin supervisión, con un corpus de texto grande\n",
    "* **Fine-tuning:** $\\;$ ajustamos el modelo pre-entrenado a la tarea con un conjunto pequeño y supervisado\n",
    "\n",
    "**Modelos causales y no-causales:**\n",
    "* Los causales generan texto\n",
    "* Los no-causales no generan texto; se orientan a la representación de frases con $\\boldsymbol{h}_t$ dependiente de pasado $\\boldsymbol{y}_{1:t-1}$, presente $\\boldsymbol{y}_t$ y futuro $\\boldsymbol{y}_{t+1:T}$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
