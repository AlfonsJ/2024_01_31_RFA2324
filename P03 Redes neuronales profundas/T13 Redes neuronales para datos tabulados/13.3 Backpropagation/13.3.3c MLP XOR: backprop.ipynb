{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.3.3c MLP XOR: backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; np.set_printoptions(precision=4)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])\n",
    "W = np.array([[1, 1], [1, 1]]); b1 = np.array([-1,  .5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "|$x_1$|$x_2$|$\\boldsymbol{z}=\\mathbf{W}\\boldsymbol{x}+\\boldsymbol{b}_1$|$\\boldsymbol{h}=\\operatorname{ReLU}(\\boldsymbol{z})$|$\\boldsymbol{a}=\\mathbf{V}\\boldsymbol{h}+\\boldsymbol{b}_2$|$\\hat{\\boldsymbol{y}}=\\mathcal{S}(\\boldsymbol{a})$|$-\\boldsymbol{y}^t\\log(\\hat{\\boldsymbol{y}})$|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|$0$|$0$|$(-1, 0.5)^t$|$(0, 0.5)^t$|$(0.5, -0.5)^t$|$(0.7311, 0.2689)^t$|$(0.3133, 0)^t$|\n",
    "|$0$|$1$|$(0, 1.5)^t$ |$(0, 1.5)^t$|$(-0.5, 0.5)^t$|$(0.2689, 0.7311)^t$|$(0, 0.3133)^t$|\n",
    "|$1$|$0$|$(0, 1.5)^t$ |$(0, 1.5)^t$|$(-0.5, 0.5)^t$|$(0.2689, 0.7311)^t$|$(0, 0.3133)^t$|\n",
    "|$1$|$1$|$(1, 2.5)^t$ |$(1, 2.5)^t$|$(-0.5, 0.5)^t$|$(0.2689, 0.7311)^t$|$(1.3133, 0)^t$|\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = [[-1.   0.5], [ 0.   1.5], [ 0.   1.5], [ 1.   2.5]]\n",
      "h = [[0.  0.5], [0.  1.5], [0.  1.5], [1.  2.5]]\n",
      "a = [[ 0.5 -0.5], [-0.5  0.5], [-0.5  0.5], [-0.5  0.5]]\n",
      "p = [[0.7311 0.2689], [0.2689 0.7311], [0.2689 0.7311], [0.2689 0.7311]]\n",
      "Ln = [[ 0.3133 -0.    ], [-0.      0.3133], [-0.      0.3133], [ 1.3133 -0.    ]]\n",
      "L = 0.5632616875182226\n"
     ]
    }
   ],
   "source": [
    "z = X @ W + b1; print('z =', str(z).replace('\\n',','))\n",
    "h = np.maximum(0, z); print('h =', str(h).replace('\\n',','))\n",
    "V = np.array([[1, -1], [-1, 1]]); b2 = np.array([ 1, -1])\n",
    "a = h @ V + b2; print('a =', str(a).replace('\\n',','))\n",
    "p = np.exp(a); p = np.transpose(p.T / p.sum(axis=1)); print('p =', str(p).replace('\\n',','))\n",
    "Ln = -y * np.log(p); print('Ln =', str(Ln).replace('\\n',','))\n",
    "print('L =', np.sum(Ln)/4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward\n",
    "\n",
    "**Jacobiana con respecto a $\\boldsymbol{a}$:** $\\quad\\displaystyle\\mathbf{J}_{\\mathcal{L}}(\\boldsymbol{a})=(\\boldsymbol{p}-\\boldsymbol{y})^t\\in\\mathbb{R}^{m\\times m_3}$\n",
    "\n",
    "**VJP con respecto a $\\boldsymbol{a}$:** $\\qquad\\underbrace{\\boldsymbol{u}^t}_{1\\times m}\\underbrace{\\mathbf{J}_{\\mathcal{L}}(\\boldsymbol{a})}_{m\\times m_3}=\\underbrace{\\mathbf{J}_{\\mathcal{L}}(\\boldsymbol{a})}_{1\\times m_3}\\quad$ ya que $m=1$ e, inicialmente, $u=1;\\; m_3=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ja = [[-0.2689  0.2689], [ 0.2689 -0.2689], [ 0.2689 -0.2689], [-0.7311  0.7311]]\n"
     ]
    }
   ],
   "source": [
    "Ja = p - y; print('Ja =', str(Ja).replace('\\n',','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jacobiana con respecto a $\\boldsymbol{z}$:** $\\quad\\displaystyle\\mathbf{J}_{\\boldsymbol{h}}(\\boldsymbol{z})=\\operatorname{diag}(\\varphi'(\\boldsymbol{z}))$\n",
    "\n",
    "**VJP de $\\boldsymbol{h}\\,(\\operatorname{ReLU})$ con respecto a $\\boldsymbol{z}$:** $\\qquad\\underbrace{\\mathbf{J}_{\\boldsymbol{h}}(\\boldsymbol{z})}_{m_2\\times m_1};\\;m_2=m_1=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jz = [[0. 1.], [0. 1.], [0. 1.], [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; np.set_printoptions(precision=4)\n",
    "z = np.array([[-1., 0.5], [0., 1.5], [0., 1.5], [1., 2.5]])\n",
    "Jz = np.heaviside(z, 0.0); print('Jz =', str(Jz).replace('\\n',','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respecto a la linealidad de la capa de salida $\\mathbf{V}$\n",
    "$$\\boldsymbol{\\nabla}_{\\mathbf{V}}\\mathcal{L}%\n",
    "=\\biggl[\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{V}}\\biggr]_{1,:}%\n",
    "=\\biggl[\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{a}}\\frac{\\partial\\boldsymbol{a}}{\\partial\\mathbf{V}}\\biggr]_{1,:}%\n",
    "=\\biggl[\\boldsymbol{u}_2^t\\frac{\\partial\\boldsymbol{a}}{\\partial\\mathbf{V}}\\biggr]_{1,:}%\n",
    "=\\boldsymbol{u}_2\\boldsymbol{h}^t\\in\\mathbb{R}^{C\\times K}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.2689  0.2689], shape=(2,), dtype=float32) tf.Tensor([0.  0.5], shape=(2,), dtype=float32) tf.Tensor(\n",
      "[[-0.     -0.1345]\n",
      " [ 0.      0.1345]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(u2[0], h[0, :], tf.tensordot(u2[0], h[0, :], axes=0))\n",
    "#gV = u2[0] @ tf.transpose(h[0])\n",
    "#print(u2[0], tf.transpose(h[0]), gV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z = L1_preact(X); print('z =', str(z).replace('\\n',''))\n",
    "h = L1(X); print('h =', str(h).replace('\\n',''))\n",
    "L2logits = L2_preact(h); print('logits =', str(L2logits).replace('\\n','')) # softmax\n",
    "y_pred = L2(h); print('y_pred =', str(y_pred).replace('\\n',''))\n",
    "eL2 = np.exp(L2logits)\n",
    "y_pred_alt = np.transpose(eL2.T / eL2.sum(axis=1))\n",
    "print('y_pred_alt =', str(y_pred_alt).replace('\\n','')) # softmax a mano\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "print('loss =', loss(y, y_pred)); \n",
    "loss_alt_all = -y * np.log(y_pred) # loss a mano\n",
    "print('loss_alt_data =', str(loss_alt_all).replace('\\n',''))\n",
    "print('loss_alt =', tf.reduce_sum(loss_alt_all) / 4.) # SUM_OVER_BATCH_SIZE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
