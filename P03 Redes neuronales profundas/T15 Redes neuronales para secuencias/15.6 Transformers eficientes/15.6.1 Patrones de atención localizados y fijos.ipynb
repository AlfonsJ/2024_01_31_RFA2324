{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.6.1 Patrones de atención localizados y fijos\n",
    "\n",
    "**Idea básica:** $\\;$ el mecanismo de atención se restringe mediante una ventana localizada no aprendible y fija, esto es, cada token solo atiende a un conjunto pre-seleccionado de tokens\n",
    "\n",
    "**Ejemplo:** $\\;$ cada secuencia se divide en $K$ bloques de longitud $\\frac{N}{K}$ y la atención se lleva a cabo dentro de cada bloque, por lo que la complejidad espacial y temporal se reduce de $O(N^2)$ a $O(\\frac{N^2}{K})$\n",
    "\n",
    "**Patrones fijos generales:** $\\;$ los patrones de atención pueden no ser solo bloques, sino que también son posibles otras aproximaciones como ventanas a saltos o dilatadas, o patrones híbridos combinados\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
