{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14.2.4 Capas de normalización\n",
    "\n",
    "* **Objetivo:** $\\;$ añadir capas extra para evitar el problema de los gradientes que desaparecen o explotan en redes profundas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2.4.1 Normalización batch\n",
    "\n",
    "* **Normalización batch:** $\\;$ normaliza la distribución de las activaciones de una capa a media nula y varianza unitaria a nivel de minibatch\n",
    "  * El vector de activación $\\boldsymbol{z}_n$ para la muestra $n$ se reemplaza por $\\;\\tilde{\\boldsymbol{z}}_n=\\boldsymbol{\\gamma}\\odot\\hat{\\boldsymbol{z}}_n+\\boldsymbol{\\beta},\\;$ donde $\\,\\boldsymbol{\\gamma}\\,$ y $\\,\\boldsymbol{\\beta}\\,$ son parámetros de aprendizaje y $\\,\\hat{\\boldsymbol{z}}_n$ es el vector de activación estandarizado en el batch $\\,\\mathcal{B}\\,$ ($\\epsilon>0$ pequeña para evitar problemas numéricos)\n",
    "$$\\hat{\\boldsymbol{z}}_n=\\frac{\\boldsymbol{z}_n-\\boldsymbol{\\mu}_{\\mathcal{B}}}{\\sqrt{\\boldsymbol{\\sigma}_{\\mathcal{B}}^2+\\epsilon}}\\qquad\\text{con}\\quad%\n",
    "\\boldsymbol{\\mu}_{\\mathcal{B}}=\\frac{1}{\\lvert{\\mathcal{B}}\\rvert}\\sum_{\\boldsymbol{z}\\in\\mathcal{B}}\\boldsymbol{z}%\n",
    "\\quad\\text{y}\\quad%\n",
    "\\boldsymbol{\\sigma}_{\\mathcal{B}}^2=\\frac{1}{\\lvert{\\mathcal{B}}\\rvert}%\n",
    "\\sum_{\\boldsymbol{z}\\in\\mathcal{B}}(\\boldsymbol{z}-\\boldsymbol{\\mu}_{\\mathcal{B}})^2$$\n",
    "* **Normalización batch es diferenciable:** $\\;$ permite retropropagar gradientes a la entrada de la capa y parámetros $\\boldsymbol{\\gamma}$ y $\\boldsymbol{\\beta}$\n",
    "* **Internal covariate shift:** $\\;$ a diferencia de la media y varianza de la capa de entrada, que se calculan una vez (para datos estáticos), las medias y varianzas de otras capas deben recalcularse en cada minibatch ya que los parámetros cambian durante el aprendizaje\n",
    "* **Medias y varianzas en inferencia:** $\\;$ no se calculan a partir del minibatch (unitario) de test, sino que se estiman a partir de todos los datos, tras finalizar el aprendizaje, y se \"congelan\" para inferencia\n",
    "* **Fused batchnorm:** $\\;$ combina una capa batchnorm congelada con su capa previa por eficiencia\n",
    "  * Si la previa calcula $\\mathbf{X}\\mathbf{W}+\\boldsymbol{b}$, la batchnorm es $\\boldsymbol{\\gamma}\\odot(\\mathbf{X}\\mathbf{W}+\\boldsymbol{b}-\\boldsymbol{\\mu})/\\boldsymbol{\\sigma}+\\boldsymbol{\\beta}$ o, en combinación,\n",
    "$$\\mathbf{X}\\mathbf{W}'+\\boldsymbol{b}'\\qquad\\text{con}\\quad%\n",
    "\\mathbf{W}'=\\boldsymbol{\\gamma}\\odot\\mathbf{W}/\\boldsymbol{\\sigma}\\quad\\text{y}\\quad%\n",
    "\\boldsymbol{b}'=\\boldsymbol{\\gamma}\\odot(\\boldsymbol{b}-\\boldsymbol{\\mu})/\\boldsymbol{\\sigma}+\\boldsymbol{\\beta}$$\n",
    "* **Batch renormalization:** $\\;$ variante para mejorar el comportamiento de batchnorm en entrenamiento con minibatches pequeños"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2.4.2 Otros tipos de capa de normalización\n",
    "\n",
    "* **Estandarización en dimensiones distintas al batch:**\n",
    "    * $z_i$ se define en un subconjunto de valores del tensor en su dimensión $i$, $\\mathcal{S}_i$\n",
    "    * El valor de $z_i$ se reemplaza por $\\,\\tilde{z}_i=\\gamma_c\\hat{z}_i+\\beta_c,\\,$ donde $c$ es el canal correspondiente al índice $i$ y $\\,\\hat{z}_i$ es el valor estandarizado\n",
    "$$\\hat{z}_i=\\frac{z_i-\\mu_i}{\\sqrt{\\sigma_i+\\epsilon}}\\qquad\\text{con}\\quad%\n",
    "\\mu_i=\\frac{1}{\\lvert{\\mathcal{S}_i}\\rvert}\\sum_{k\\in\\mathcal{S}_i}z_k\\quad\\text{y}\\quad%\n",
    "\\sigma_i^2=\\frac{1}{\\lvert{\\mathcal{S}_i}\\rvert}\\sum_{k\\in\\mathcal{S}_i}(z_k-\\mu_i)^2$$\n",
    "\n",
    "* **Batch/layer/instance/group norm:** $\\;$ posiciones agrupadas en azul\n",
    "<div align=\"center\">\n",
    "<table><tr><td style=\"border: none;\"><img src=\"Figure_14.14.png\"/></td></tr></table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2.4.3 Redes libres de normalización\n",
    "\n",
    "* Propuesta reciente para entrenar redes residuales profundas sin capas de normalización con recortado de gradientes adaptativo\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
