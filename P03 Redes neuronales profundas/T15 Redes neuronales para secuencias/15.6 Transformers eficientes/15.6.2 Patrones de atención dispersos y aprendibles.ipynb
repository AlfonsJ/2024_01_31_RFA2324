{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.6.2 Patrones de atención dispersos y aprendibles\n",
    "\n",
    "* **Idea básica:** $\\;$ como en los patrones localizados y fijos, el mecanismo de atención se restringe a pares de tokens dentro de una única partición de algún particionamiento de todos los tokens, pero dichos particionamientos no son fijos, sino que se entrenan\n",
    "\n",
    "* **Hashing:** $\\;$ los tokens se hashean y cada cubo de hashing corresponde a una partición distinta\n",
    "\n",
    "* **Reformer:** $\\;$ aplica **Locality Sensitive Hashing (LSH)** con coste temporal $O(NM^2\\log(M))$ para embeddings de dimensión $M$\n",
    "\n",
    "* **Clustering:** $\\;$ aplica $K$-means para reducir el coste a $O(\\frac{N^2}{K})$ si los clústers son del mismo tamaño; usualmente, $K=\\sqrt{N}$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
