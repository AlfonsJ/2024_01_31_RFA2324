{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.2.9 Ejercicios de regresión logística binaria"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1:** Sea un problema de clasificación en dos clases, $y\\in\\{0,1\\}$, y sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b))$ un modelo de regresión logística binaria con $\\boldsymbol{w}=(3,3)^t$ y $b=0$. Determina la probabilidad de que $\\boldsymbol{x}=(0.5,0.5)^t$ pertenezca a la clase $1$ según el modelo dado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})%\n",
    "=\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b)=\\sigma((3, 3)(0.5, 0.5)^t+0)=\\dfrac{1}{1+e^{-3}}=0.9526$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2:** Sea un problema de clasificación en dos clases, $y\\in\\{0,1\\}$, y sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b))$ un modelo de regresión logística binaria con $\\boldsymbol{w}=(3,3)^t$ y $b=0$. Determina la logit, $f(\\boldsymbol{x}; \\boldsymbol{\\theta})$, así como la frontera y regiones de decisión que induce (con pérdida 0-1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad$ Logit: $\\quad f(\\boldsymbol{x}; \\boldsymbol{\\theta})=b+\\boldsymbol{w}^t\\boldsymbol{x}=3 x_1+3 x_2$\n",
    "\n",
    "Frontera: $\\quad 3 x_1+3 x_2=0\\to x_2=-x_1$\n",
    "\n",
    "Regiones de decisión:\n",
    "$$\\begin{align*}\n",
    "  \\mathcal{R}_1%\n",
    "  &=\\{\\boldsymbol{x}:p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})>p(y=0\\mid\\boldsymbol{x};\\boldsymbol{\\theta})\\}\\\\%\n",
    "  &=\\left\\{\\boldsymbol{x}:\\frac{p(y=1\\mid\\boldsymbol{x};\\boldsymbol{\\theta})}{p(y=0\\mid\\boldsymbol{x};\\boldsymbol{\\theta})}>1\\right\\}\\\\%\n",
    "  &=\\{\\boldsymbol{x}:f(\\boldsymbol{x};\\boldsymbol{\\theta})>0\\}\\\\%\n",
    "  &=\\{\\boldsymbol{x}:3 x_1+3 x_2>0\\}\\\\%\n",
    "  &=\\{\\boldsymbol{x}:x_2>-x_1\\}\\\\[3mm]%\n",
    "  \\mathcal{R}_0%\n",
    "  &=\\{\\boldsymbol{x}:x_2<-x_1\\}%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3:** Sea $\\mathcal{D}=\\{(\\boldsymbol{x}_n,y_n): \\boldsymbol{x}_n\\in\\mathbb{R}^D, y_n\\in\\{0,1\\}\\}$, un conjunto de muestras no linealmente separables. Se quiere aprender un modelo de clasificación $p(y\\mid\\boldsymbol{x};\\boldsymbol{\\theta})$ que las clasifique sin error (pérdida 0-1 nula). Indica la respuesta correcta:\n",
    "1. Podemos usar regresión logística binaria convencional, esto es, con $p(y=1\\mid\\boldsymbol{x})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}+b))$.\n",
    "2. No podemos usar regresión logística binaria convencional, pero sí regresión logística binaria con una función de preproceso, $\\phi(\\boldsymbol{x})$, que linearice las muestras.\n",
    "3. No podemos usar regresión logística binaria, convencional o no, pues necesitamos un modelo de clasificación multi-clase.\n",
    "4. No podemos usar regresión logística binaria, convencional o no, pues es un modelo lineal y necesitamos uno no lineal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 2; la 4 es falsa porque puede haber una función de preproceso que linearice las muestras."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 4:** $\\;$ Sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{w})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}))$, $y\\in\\{0,1\\}$, un modelo de regresión logística binaria en notación compacta (homogénea). Sea el conjunto de datos de entrenamiento $\\mathcal{D}=\\{(\\boldsymbol{x}_n,y_n)\\}$, donde $\\boldsymbol{x}_n\\in\\mathbb{R}^D$ y $y_n\\in\\{0,1\\}$. La neg-log-verosimilitud de $\\boldsymbol{w}$ con respecto a $\\mathcal{D}$ es:\n",
    "$$\\operatorname{NLL}(\\boldsymbol{w})=-\\frac{1}{N}\\sum\\nolimits_n y_n\\log\\mu_n+(1-y_n)\\log(1-\\mu_n)$$\n",
    "donde $\\mu_n=\\sigma(a_n)$ con log-odds $a_n=\\boldsymbol{w}^t\\boldsymbol{x}_n$. Demuestra que el gradiente de la neg-log-verosimilitud es:\n",
    "$$\\frac{\\partial\\operatorname{NLL}}{\\partial\\boldsymbol{w}^t}=\\frac{1}{N}\\sum\\nolimits_n(\\mu_n-y_n)\\boldsymbol{x}_n$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\operatorname{NLL}}{\\partial w_d}%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "y_n\\frac{\\partial}{\\partial w_d}\\log\\mu_n%\n",
    "+(1-y_n)\\frac{\\partial}{\\partial w_d}\\log(1-\\mu_n)\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "y_n\\frac{1}{\\mu_n}\\frac{\\partial}{\\partial w_d}\\mu_n%\n",
    "-(1-y_n)\\frac{1}{1-\\mu_n}\\frac{\\partial}{\\partial w_d}\\mu_n\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "\\left[\\frac{y_n}{\\mu_n}-\\frac{1-y_n}{1-\\mu_n}\\right]%\n",
    "\\frac{\\partial}{\\partial w_d}\\mu_n\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "\\frac{y_n(1-\\mu_n)-(1-y_n)\\mu_n}{\\mu_n(1-\\mu_n)}%\n",
    "\\frac{\\partial}{\\partial a_n}\\sigma(a_n)\\frac{\\partial a_n}{\\partial w_d}\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n%\n",
    "\\frac{y_n-y_n\\mu_n-\\mu_n+y_n\\mu_n}{\\mu_n(1-\\mu_n)}%\n",
    "\\mu_n(1-\\mu_n)x_{nd}\\\\%\n",
    "&=-\\frac{1}{N}\\sum\\nolimits_n(y_n-\\mu_n)x_{nd}%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 5:** Sea $p(y\\mid\\boldsymbol{x};\\boldsymbol{w})=\\operatorname{Ber}(y\\mid\\sigma(\\boldsymbol{w}^t\\boldsymbol{x}))$, $y\\in\\{0,1\\}$, un modelo de regresión logística binaria en notación compacta (homogénea). Sea el conjunto de datos de entrenamiento $\\;\\mathcal{D}=\\{(\\boldsymbol{x}_n,y_n)\\}$, donde $\\;\\boldsymbol{x}_n\\in\\mathbb{R}^D\\;$ y $\\;y_n\\in\\{0,1\\}$. Sabemos que el gradiente de la neg-log-verosimilitud de $\\boldsymbol{w}$ es:\n",
    "$$\\frac{\\partial\\operatorname{NLL}}{\\partial\\boldsymbol{w}^t}=\\frac{1}{N}\\sum\\nolimits_n(\\mu_n-y_n)\\boldsymbol{x}_n%\n",
    "\\qquad\\text{donde}\\quad%\n",
    "\\mu_n=\\sigma(a_n)\\quad\\text{con log-odds}\\quad%\n",
    "a_n=\\boldsymbol{w}^t\\boldsymbol{x}_n$$\n",
    "Supón que estamos aplicando SGD con minibatch de talla $1$ para minimizar la $\\operatorname{NLL}$. Más concretamente, nos hallamos en la iteración $i$ con $\\boldsymbol{w}_i=(-1, 0, -1)^t$, $\\eta_i=0.1$ y el minibatch actual únicamente incluye la muestra $\\boldsymbol{x}_n=(1, 1, 1)^t$, de clase $y_n=1$. Determina $\\boldsymbol{w}_{i+1}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad p(y_n=1\\mid\\boldsymbol{x}_n;\\boldsymbol{w}_t)=\\mu_n=\\sigma(\\boldsymbol{w}_t^t\\boldsymbol{x}_n)=\\sigma(-2)=\\dfrac{1}{1+e^2}=0.12$\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{w}_{i+1}%\n",
    "&=\\boldsymbol{w}_i-\\eta_i\\,(\\mu_n-y_n)\\boldsymbol{x}_n\\\\%\n",
    "&=(-1, 0, -1)^t+0.1\\,(1-0.12)(1, 1, 1)^t\\\\%\n",
    "&=(-1, 0, -1)^t+0.09\\,(1, 1, 1)^t\\\\%\n",
    "&=(-0.91, 0.09, -0.91)^t%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 6:** Sea el algoritmo Perceptrón (en clasificación binaria) aplicado a una muestra $(\\boldsymbol{x}_n,y_n)$ para actualizar el vector de parámetros actual, $\\boldsymbol{w}_i$ (supón $x_{n0}=1$ y $b=w_{i0}$) con factor de aprendizaje $\\eta_i=1$. Si $y_n=0$ y $\\hat{y}_n=\\mathbb{I}(\\boldsymbol{w}_i^t\\boldsymbol{x}>0)=1$, entonces:\n",
    "1. $\\boldsymbol{w}_{i+1}=\\boldsymbol{w}_i$\n",
    "2. $\\boldsymbol{w}_{i+1}=\\boldsymbol{w}_i+\\boldsymbol{x}_n$\n",
    "3. $\\boldsymbol{w}_{i+1}=\\boldsymbol{w}_i-\\boldsymbol{x}_n$\n",
    "4. Ninguna de las anteriores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** La 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 7:** Repite el ejercicio 5 con el algoritmo Perceptrón en lugar de descenso por gradiente estocástico."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\quad p(y_n=1\\mid\\boldsymbol{x}_n;\\boldsymbol{w}_t)=\\mu_n=H(\\boldsymbol{w}_t^t\\boldsymbol{x}_n)=H(-2)=0\\quad\\to\\quad\\hat{y}_n=0$\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{w}_{t+1}%\n",
    "&=\\boldsymbol{w}_t-\\eta_t\\,(\\hat{y}_n-y_n)\\boldsymbol{x}_n\\\\%\n",
    "&=(-1, 0, -1)^t+0.1\\,(1-0)(1, 1, 1)^t\\\\%\n",
    "&=(-1, 0, -1)^t+0.1\\,(1, 1, 1)^t\\\\%\n",
    "&=(-0.9, 0.1, -0.9)^t%\n",
    "\\end{align*}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
