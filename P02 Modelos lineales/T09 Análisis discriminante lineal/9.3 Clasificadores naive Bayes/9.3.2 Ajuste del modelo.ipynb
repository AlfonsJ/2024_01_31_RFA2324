{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste del modelo\n",
    "\n",
    "**Log-verosimilitud conjunta:** $\\;\\boldsymbol{\\theta}'=\\{\\boldsymbol{\\theta}_c\\},\\,$ $\\boldsymbol{\\theta}_c=\\{\\boldsymbol{\\theta}_{dc}\\}$\n",
    "$$\\begin{align*}\n",
    "\\log p(\\mathcal{D}\\mid\\boldsymbol{\\theta})%\n",
    "&=\\sum_c N_c\\log\\pi_c + \\sum_c\\sum_{n:y_n=c}\\log p(\\boldsymbol{x}_n\\mid y=c,\\boldsymbol{\\theta}_c)\\\\%\n",
    "&=\\sum_c N_c\\log\\pi_c + \\sum_d\\sum_c\\sum_{n:y_n=c}\\log p(x_{nd}\\mid y=c,\\boldsymbol{\\theta}_{dc})%\n",
    "\\end{align*}$$\n",
    "\n",
    "**Maximización en $\\boldsymbol{\\theta}'$:** $\\;\\hat{\\boldsymbol{\\theta}}'\\,$ puede hallarse mediante maximización separada en\n",
    "cada $\\boldsymbol{\\theta}_{dc}$ (con restricciones posiblemente)\n",
    "$$\\hat{\\boldsymbol{\\theta}}'=\\operatorname*{argmax}_{\\boldsymbol{\\theta}'\\in\\mathcal{C}_{\\boldsymbol{\\theta}'}}\\sum_d\\sum_c\\sum_{n:y_n=c}\\log p(x_{nd}\\mid y=c,\\boldsymbol{\\theta}_{dc})%\n",
    "\\;\\Leftrightarrow\\;%\n",
    "\\hat{\\boldsymbol{\\theta}}_{dc}=\\operatorname*{argmax}_{\\boldsymbol{\\theta}_{dc}\\in\\mathcal{C}_{\\boldsymbol{\\theta}_{dc}}}\\sum_{n:y_n=c}\\log p(x_{nd}\\mid y=c,\\boldsymbol{\\theta}_{dc})\\quad\\text{para todo $d$ y $c$}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximización: Bernoulli\n",
    "Si las características son binarias, $x_{nd}\\in\\{0,1\\}$, es fácil comprobar que:\n",
    "$$\\hat{\\theta}_{dc}=\\frac{N_{dc}}{N_c}%\n",
    "\\quad\\text{con}\\quad%\n",
    "N_{dc}=\\sum_{n:y_n=c}\\mathbb{I}(x_{nd}=1)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** $\\;C=2$, $\\;\\pi_1=\\pi_2=0.5$, $\\;D=2$, \n",
    "$\\;\\boldsymbol{\\theta}=[\\boldsymbol{\\theta}_1;\\boldsymbol{\\theta}_2]$, $\\;\\boldsymbol{\\theta}_1=(0.7, 0.3)^t$, $\\;\\boldsymbol{\\theta}_2=(0.2, 0.8)^t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta1:  [0.63157895 0.33333333]  theta2:  [0.25581395 0.8372093 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multinomial, bernoulli\n",
    "N = 100 # >=2 para tener al menos un dato por clase\n",
    "yy = multinomial(1, [0.5, 0.5]).rvs(N - 2)\n",
    "N1 = yy[yy[:, 0] == 1].shape[0] + 1\n",
    "xxy1 = np.hstack((np.vstack(bernoulli(0.7).rvs(N1)), np.vstack(bernoulli(0.3).rvs(N1))))\n",
    "theta1 = xxy1.mean(axis=0)\n",
    "N2 = N - N1\n",
    "xxy2 = np.hstack((np.vstack(bernoulli(0.2).rvs(N2)), np.vstack(bernoulli(0.8).rvs(N2))))\n",
    "theta2 = xxy2.mean(axis=0)\n",
    "print(\"theta1: \", theta1, \" theta2: \", theta2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximización: categórica\n",
    "Si las características son categóricas, $\\;x_{nd}\\in\\{1,\\dotsc,K\\}$, es fácil comprobar que:\n",
    "$$\\hat{\\theta}_{dck}=\\frac{N_{dck}}{N_c}%\n",
    "\\quad\\text{con}\\quad%\n",
    "N_{dck}=\\sum_{x_{nd}\\in\\mathcal{D}_{dc}}\\mathbb{I}(x_{nd}=k)$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** $\\;C=2$, $\\;\\pi_1=\\pi_2=0.5$, $\\;D=2$, $\\;K_1=K_2=3$\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{\\theta}_{11}&=(0.6, 0.2, 0.2)^t & \\boldsymbol{\\theta}_{12}&=(0.3, 0.4, 0.3)^t\\\\\n",
    "\\boldsymbol{\\theta}_{21}&=(0.1, 0.3, 0.6)^t & \\boldsymbol{\\theta}_{22}&=(0.3, 0.2, 0.5)^t%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta11:  [0.63461538 0.09615385 0.26923077]  theta12:  [0.35416667 0.41666667 0.22916667]\n",
      "theta21:  [0.13461538 0.26923077 0.59615385]  theta22:  [0.41666667 0.25       0.33333333]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multinomial\n",
    "N = 100 # >=2 para tener al menos un dato por clase\n",
    "yy = multinomial(1, [0.5, 0.5]).rvs(N - 2)\n",
    "N1 = yy[yy[:, 0] == 1].shape[0] + 1\n",
    "N2 = N - N1\n",
    "theta11 = np.vstack(multinomial(1, [0.6, 0.2, 0.2]).rvs(N1)).mean(axis=0)\n",
    "theta21 = np.vstack(multinomial(1, [0.1, 0.3, 0.6]).rvs(N1)).mean(axis=0)\n",
    "theta12 = np.vstack(multinomial(1, [0.3, 0.4, 0.3]).rvs(N2)).mean(axis=0)\n",
    "theta22 = np.vstack(multinomial(1, [0.3, 0.2, 0.5]).rvs(N2)).mean(axis=0)\n",
    "print(\"theta11: \", theta11, \" theta12: \", theta12)\n",
    "print(\"theta21: \", theta21, \" theta22: \", theta22)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximización: Gaussiana\n",
    "Si las características son reales que siguen una distribución normal, $\\;x_{nd}\\in\\mathbb{R}$, es fácil comprobar que:\n",
    "$$\\begin{align*}\n",
    "\\hat{\\mu}_{dc}&=\\frac{1}{N_c}\\sum_{x_{nd}\\in\\mathcal{D}_{dc}}x_{nd}\\\\%\n",
    "\\hat{\\sigma}_{dc}^2&=\\frac{1}{N_c}\\sum_{x_{nd}\\in\\mathcal{D}_{dc}}(x_{nd}-\\hat{\\mu}_{dc})^2%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** $\\;C=2$, $\\;\\pi_1=\\pi_2=0.5$, $\\;D=2$,\n",
    "$\\;\\boldsymbol{\\theta}=[\\boldsymbol{\\theta}_1;\\boldsymbol{\\theta}_2]$\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{\\theta}_1&=(\\boldsymbol{\\mu}_1,\\mathbf{\\Sigma}_1)%\n",
    "&\\boldsymbol{\\mu}_1&=(\\mu_{11},\\mu_{21})^t=(-2,0)^t%\n",
    "&\\mathbf{\\Sigma}_1&=\\operatorname{diag}(\\sigma_{11}^2, \\sigma_{21}^2)=\\mathbf{I}_2\\\\%\n",
    "\\boldsymbol{\\theta}_2&=(\\boldsymbol{\\mu}_2,\\mathbf{\\Sigma}_2)%\n",
    "&\\boldsymbol{\\mu}_2&=(\\mu_{12},\\mu_{22})^t=(2,0)^t%\n",
    "&\\mathbf{\\Sigma}_2&=\\operatorname{diag}(\\sigma_{12}^2, \\sigma_{22}^2)=\\mathbf{I}_2%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu1:  [-2.04576436  0.18229609]  Sigma1:  [0.89499539 1.12635571]\n",
      "mu2:  [ 2.0795843  -0.18044901]  Sigma2:  [1.00070955 1.10808814]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multinomial, multivariate_normal\n",
    "N = 100 # >=2 para tener al menos un dato por clase\n",
    "yy = multinomial(1, [0.5, 0.5]).rvs(N - 2)\n",
    "N1 = yy[yy[:, 0] == 1].shape[0] + 1\n",
    "N2 = N - N1\n",
    "xxy1 = multivariate_normal([-2, 0], np.eye(2)).rvs(N1)\n",
    "mu1 = xxy1.mean(axis=0)\n",
    "Sigma1 = np.var(xxy1, axis=0)\n",
    "xxy2 = multivariate_normal([2, 0], np.eye(2)).rvs(N2)\n",
    "mu2 = xxy2.mean(axis=0)\n",
    "Sigma2 = np.var(xxy2, axis=0)\n",
    "print(\"mu1: \", mu1, \" Sigma1: \", Sigma1)\n",
    "print(\"mu2: \", mu2, \" Sigma2: \", Sigma2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
