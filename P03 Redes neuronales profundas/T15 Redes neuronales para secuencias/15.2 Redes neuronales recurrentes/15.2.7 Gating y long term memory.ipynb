{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.2.7 Gating y long term memory\n",
    "\n",
    "**Solución al problema de los gradientes que desaparecen en entrenamiento de \"vanilla\" RNNs:** $\\;$ actualizar los estados ocultos de manera aditiva, al estilo de las redes residuales en visión\n",
    "* **Gated recurrent unit (GRU, 2014):** $\\;$ aprende con un mecanismo de reinicio o actualización de estado que llamamos **gating unit**\n",
    "* **Long short term memory (LSTM, 1997):** $\\;$ puede verse como una versión sofistacada de una GRU\n",
    "* **Notación:** $\\;$ entrada $\\,\\mathbf{X}_t\\in\\mathbb{R}^{N\\times D}$, con batch y vocabulario de tallas $N$ y $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2.7.1 Gated recurrent unit (GRU)\n",
    "\n",
    "<div><table><tr>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top;\" width=550>\n",
    "\n",
    "**Estado oculto:** $\\;\\mathbf{H}_t\\in\\mathbb{R}^{N\\times H}$, con $H$ unidades ocultas\n",
    "\n",
    "**Reset gate:** $\\;\\mathbf{R}_t=\\boldsymbol{\\sigma}(\\mathbf{X}_t\\mathbf{W}_{xr}+\\mathbf{H}_{t-1}\\mathbf{W}_{hr}+\\boldsymbol{b}_r)$\n",
    "\n",
    "**Update gate:** $\\;\\mathbf{Z}_t=\\boldsymbol{\\sigma}(\\mathbf{X}_t\\mathbf{W}_{xz}+\\mathbf{H}_{t-1}\\mathbf{W}_{hz}+\\boldsymbol{b}_z)$\n",
    "\n",
    "**Candidato:** $\\;\\tilde{\\mathbf{H}}_t=\\tanh(\\mathbf{X}_t\\mathbf{W}_{xh}+(\\mathbf{R}_t\\odot\\mathbf{H}_{t-1})\\mathbf{W}_{hh}+\\boldsymbol{b}_h)$\n",
    "* Combina la nueva entrada $\\mathbf{X}_t$ con la memoria retenida $\\mathbf{R}_t\\odot\\mathbf{H}_{t-1}$\n",
    "* Si retiene memoria, $\\mathbf{R}_t\\approx\\boldsymbol{1}$, actúa como una RNN\n",
    "* Si no retiene memoria, $\\mathbf{R}_t\\approx\\boldsymbol{0}$, actúa como un MLP\n",
    "\n",
    "**Nuevo:** $\\;\\mathbf{H}_t=\\mathbf{Z}_t\\odot\\mathbf{H}_{t-1}+(1-\\mathbf{Z}_t)\\odot\\tilde{\\mathbf{H}}_t$\n",
    "* Si $Z_{td}=1$, se comporta como memoria long-term, $H_{t,d}=H_{t-1,d}$\n",
    "* Si $Z_{td}=0$, se comporta según el candidato, $H_{t,d}=\\tilde{H}_{t,d}$\n",
    "\n",
    "</td><td style=\"border: none;\"><img src=\"Figure_15.10.png\" width=450></td>\n",
    "</tr></table></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2.7.2 Long short term memory (LSTM)\n",
    "\n",
    "<div><table><tr>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top;\" width=550>\n",
    "\n",
    "**Cell memory/state:** $\\;\\mathbf{C}_t$, aumenta el estado oculto $\\mathbf{H}_t$\n",
    "\n",
    "**Output gate:** $\\;\\mathbf{O}_t=\\boldsymbol{\\sigma}(\\mathbf{X}_t\\mathbf{W}_{xo}+\\mathbf{H}_{t-1}\\mathbf{W}_{ho}+\\boldsymbol{b}_o)$\n",
    "\n",
    "**Input gate:** $\\;\\mathbf{I}_t=\\boldsymbol{\\sigma}(\\mathbf{X}_t\\mathbf{W}_{xi}+\\mathbf{H}_{t-1}\\mathbf{W}_{hi}+\\boldsymbol{b}_i)$\n",
    "\n",
    "**Forget gate:** $\\;\\mathbf{F}_t=\\boldsymbol{\\sigma}(\\mathbf{X}_t\\mathbf{W}_{xf}+\\mathbf{H}_{t-1}\\mathbf{W}_{hf}+\\boldsymbol{b}_f)$\n",
    "\n",
    "**Celda candidata:** $\\;\\tilde{\\mathbf{C}}_t=\\tanh(\\mathbf{X}_t\\mathbf{W}_{xc}+\\mathbf{H}_{t-1}\\mathbf{W}_{hc}+\\boldsymbol{b}_c)$\n",
    "\n",
    "**Celda nueva:** $\\;\\mathbf{C}_t=\\mathbf{F}_t\\odot\\mathbf{C}_{t-1}+\\mathbf{I}_t\\odot\\tilde{\\mathbf{C}}_t$\n",
    "* Si $\\mathbf{I}_t=\\boldsymbol{0}$ y $\\mathbf{F}_t=\\boldsymbol{1}$, es memoria long-term, $\\mathbf{C}_t=\\mathbf{C}_{t-1}$\n",
    "\n",
    "**Estado oculto:** $\\;\\mathbf{H}_t=\\mathbf{O}_t\\odot\\tanh(\\mathbf{C}_t)$\n",
    "* Si $\\mathbf{O}_t=\\boldsymbol{1}$, es una transformación inmediata de $\\mathbf{C}_t$\n",
    "* **Short-term memory:** $\\;$ $\\mathbf{H}_t$ es estado oculto (de entrada) en $t+1$\n",
    "\n",
    "</td><td style=\"border: none;\"><img src=\"Figure_15.11.png\" width=450></td>\n",
    "</tr></table></div>\n",
    "\n",
    "**Conexiones peephole:** $\\;$ puertas con aceso al estado anterior"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
