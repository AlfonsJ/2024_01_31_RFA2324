{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting\n",
    "\n",
    "**Gradient boosting:** $\\;$ FSAM visto como descenso por gradiente para un problema de minimización en un espacio funcional\n",
    "$$\\hat{\\boldsymbol{f}}%\n",
    "=\\operatorname*{argmin}_{\\boldsymbol{f}}\\mathcal{L}(\\boldsymbol{f})%\n",
    "\\quad\\text{con}\\quad%\n",
    "\\mathcal{L}(\\boldsymbol{f})=\\sum_{i=1}^N \\ell(y_i,f(\\boldsymbol{x}_i))$$\n",
    "\n",
    "**Funciones base simplificadas:** $\\;$ valores en el conjunto de entrenamiento, $\\;\\boldsymbol{f}=(f(\\boldsymbol{x}_1),\\dotsc,f(\\boldsymbol{x}_N))^t$\n",
    "\n",
    "**Descenso por gradiente:** $\\;$ escoge la \"dirección\" de máximo descenso, esto es, la del neg-gradiente de $\\mathcal{L}(\\boldsymbol{f})$ en $\\boldsymbol{f}_{m-1}$, $\\boldsymbol{g}_m$\n",
    "$$\\boldsymbol{f}_m=\\boldsymbol{f}_{m-1}-\\beta_m \\boldsymbol{g}_m\n",
    "\\qquad\\text{con}\\qquad%\n",
    "g_{im}=\\left[\\frac{\\partial\\ell(y_i,f(\\boldsymbol{x}_i))}{\\partial f(\\boldsymbol{x}_i)}\\right]_{f_{m-1}(\\boldsymbol{x}_i)}$$\n",
    "\n",
    "**Factor de aprendizaje:** $\\;\\beta_m$ puede escogerse por búsqueda lineal\n",
    "\n",
    "**Funciones base generalizadas:** $\\;$ para poder generalizar, se ajusta un aprendiz débil al neg-gradiente con pérdida cuadrática\n",
    "$$F_m=\\operatorname*{argmin}_F\\sum_{i=1}^N(-g_{im}-F(\\boldsymbol{x}_i))^2$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo básico\n",
    "\n",
    "El algoritmo básico prescinde de $\\beta_m$ pero incluye un **shrinkage factor** $0<\\nu\\leq 1$ para facilitar la regularización:\n",
    "1. Inicializar $\\,f_0(\\boldsymbol{x})=\\operatorname{argmin}_F\\sum_{i=1}^N\\ell(y_i,F(\\boldsymbol{x}_i))$\n",
    "2. **for** $\\;m=1:M\\;$ **do**\n",
    "3. $\\quad$ Calcular el neg-gradiente o (pseudo-)**residuo** $\\,r_{im}=-\\left[\\dfrac{\\partial\\ell(y_i,f(\\boldsymbol{x}_i))}{\\partial f(\\boldsymbol{x}_i)}\\right]_{f_{m-1}(\\boldsymbol{x}_i)}$\n",
    "4. $\\quad$ Usar el aprendiz débil para hallar $\\,F_m=\\operatorname*{argmin}_F\\sum_{i=1}^N(r_{im}-F(\\boldsymbol{x}_i))^2$\n",
    "5. $\\quad$ Actualizar $\\,f_m(\\boldsymbol{x})=f_{m-1}(\\boldsymbol{x})+\\nu F_m(\\boldsymbol{x})$\n",
    "6. Devolver $\\,f(\\boldsymbol{x})=f_M(\\boldsymbol{x})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión\n",
    "\n",
    "**Salidas:** $\\;y_i\\in\\mathbb{R}$\n",
    "\n",
    "**Pérdida cuadrática o su mitad:** $\\;\\ell(y_i,f(\\boldsymbol{x}_i))=\\frac{1}{2}(y_i-f(\\boldsymbol{x}_i))^2\\quad$ (como boosting mínimo cuadrados) \n",
    "\n",
    "**Neg-gradiente cuadrático:** $\\;r_i=y_i-f(\\boldsymbol{x}_i)$\n",
    "\n",
    "**Pérdida valor absoluto:** $\\;\\ell(y_i,f(\\boldsymbol{x}_i))=\\lvert y_i-f(\\boldsymbol{x}_i)\\rvert$\n",
    "\n",
    "**Neg-gradiente valor absoluto:** $\\;r_i=\\operatorname{sgn}(y_i-f(\\boldsymbol{x}_i))$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación binaria\n",
    "\n",
    "**Salidas:** $\\;\\tilde{y}_i\\in\\{-1,+1\\}$\n",
    "\n",
    "**Pérdida exponencial:** $\\;\\ell(\\tilde{y}_i,f(\\boldsymbol{x}_i))=\\exp(-\\tilde{y}_i f(\\boldsymbol{x}_i))\\quad$ (como Adaboost)\n",
    "\n",
    "**Neg-gradiente exponencial:** $\\;r_i=\\tilde{y}_i\\exp(-\\tilde{y}_i f(\\boldsymbol{x}_i))$\n",
    "\n",
    "**Log-pérdida binaria:** $\\;\\ell(\\tilde{y}_i,f(\\boldsymbol{x}_i))=\\log(1+\\exp(-\\tilde{y}_i f(\\boldsymbol{x}_i)))\\quad$ (como LogitBoost)\n",
    "\n",
    "**Neg-gradiente log-pérdida binaria:**\n",
    "$$r_i=-\\frac{1}{1+\\exp(-\\tilde{y}_i f(\\boldsymbol{x}_i))}\\exp(-\\tilde{y}_i f(\\boldsymbol{x}_i))(-\\tilde{y}_i)%\n",
    "=\\tilde{y}_i\\frac{1}{1+\\exp(\\tilde{y}_i f(\\boldsymbol{x}_i))}%\n",
    "=\\tilde{y}_i\\sigma(-\\tilde{y}_i f(\\boldsymbol{x}_i))$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación multiclase\n",
    "\n",
    "**Salidas:** $\\;y_i\\in\\{1,\\dotsc,C\\}$\n",
    "\n",
    "**Log-pérdida:** $\\;$ se ajustan $C$ modelos aditivos, uno por cada clase, cuyas predicciones se normalizan mediante una softmax\n",
    "$$\\ell(y_i,f_1(\\boldsymbol{x}_i),\\dotsc,f_C(\\boldsymbol{x}_i))=-\\sum_c \\mathbb{I}(y_i=c)\\log \\pi_{ic}%\n",
    "\\quad\\text{con}\\quad%\n",
    "\\pi_{ic}=S(f_1(\\boldsymbol{x}_i),\\dotsc,f_C(\\boldsymbol{x}_i))_c%\n",
    "=\\frac{\\exp(f_c(\\boldsymbol{x}_i))}{\\sum_{c'=1}^C \\exp(f_{c'}(\\boldsymbol{x}_i))}$$\n",
    "\n",
    "**Neg-gradiente log-pérdida:** $\\;$ para cada clase $c$\n",
    "$$\\begin{align*}\n",
    "r_{ic}%\n",
    "&=-\\dfrac{\\partial\\ell(y_i,f_1(\\boldsymbol{x}_i),\\dotsc,f_C(\\boldsymbol{x}_i))}{\\partial f_c(\\boldsymbol{x}_i)}\\\\%\n",
    "&=\\frac{\\partial}{\\partial f_c(\\boldsymbol{x}_i)}\\sum_{\\tilde{c}} \\mathbb{I}(y_i=\\tilde{c})\\log \\pi_{i\\tilde{c}}\\\\%\n",
    "&=\\mathbb{I}(y_i=c)\\frac{1}{\\pi_{ic}}\\pi_{ic}(1-\\pi_{ic})\\\\%\n",
    "&=\\mathbb{I}(y_i=c)(1-\\pi_{ic})\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient tree boosting\n",
    "\n",
    "**Gradient tree boosting:** $\\;$ gradient boosting con árbol de regresión como aprendiz débil\n",
    "$$F_m=\\sum_{j=1}^{J_m}w_{jm}\\mathbb{I}(\\boldsymbol{x}\\in R_{jm})$$\n",
    "* $R_{jm}$ y $w_{jm}$ son la región y salida asociadas a la hoja $j$ del árbol añadido en la iteración $m$\n",
    "* La salida puede ser un escalar o, más generalmente, un vector (de probabilidades, por ejemplo)\n",
    "\n",
    "**Aprendizaje de las regiones:** $\\;$ CART sobre residuos\n",
    "\n",
    "**Aprendizaje de las salidas:** $\\;$ minimización del riesgo empírico con los datos de la hoja\n",
    "$$\\hat{w}_{jm}=\\operatorname*{argmin}_w\\sum_{\\boldsymbol{x}_i\\in R_{jm}}%\n",
    "\\ell(y_i,f_{m-1}(\\boldsymbol{x}_i)+w)$$\n",
    "\n",
    "**Aprendizaje de las salidas con pérdida cuadrática:** $\\;\\hat{w}_{jm}$ es la media empírica de los residuos de la hoja"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "**Extreme gradient boosting (XGBoost):** $\\;$ implementación muy popular de gradient tree boosting con algunos refinamientos\n",
    "* Objetivo regularizado\n",
    "* Aproximación de segundo orden de la pérdida\n",
    "* Muestreo de caracterı́sticas en nodos internos\n",
    "* Técnicas algorítmicas varias para mejorar la escabilidad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
