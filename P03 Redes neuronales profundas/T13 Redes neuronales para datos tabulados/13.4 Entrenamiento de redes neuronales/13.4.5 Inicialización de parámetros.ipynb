{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.4.5 Inicialización de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esquemas de inicialización heurísticos\n",
    "\n",
    "**Caso base:** $\\;$ inicializar los pesos $\\{w_{ij}\\}$ de una unidad lineal (sin función de activación) con $n_{\\text{in}}$ entradas (**fan-in**) y $n_{\\text{out}}$ salidas (**fan-out**) \n",
    "$$o_i=\\sum_{j=1}^{n_{\\text{in}}}w_{ij}x_j\\qquad i=1:n_{\\text{out}}$$\n",
    "\n",
    "**Inicialización Xavier o Glorot:** $\\quad w_{ij}\\sim\\mathcal{N}(0,\\sigma^2)\\quad\\text{con}\\quad\\sigma^2=\\frac{2}{n_{\\text{in}}+n_{\\text{out}}}$\n",
    "\n",
    "* Si $\\,\\mathbb{E}[x_j]=0\\,$ y $\\,\\mathbb{V}[x_j]=\\gamma^2,\\,$ con $x_j$ y $w_{ij}$ independientes:\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[o_i]&=\\sum\\nolimits_{j=1}^{n_\\text{in}}\\mathbb{E}[w_{ij}x_j]%\n",
    "=\\sum\\nolimits_{j=1}^{n_\\text{in}}\\mathbb{E}[w_{ij}]\\mathbb{E}[x_j]=0\\\\%\n",
    "\\mathbb{V}[o_i]&=\\mathbb{E}[o_i^2]-\\mathbb{E}[o_i]^2%\n",
    "=\\sum_{j=1}^{n_\\text{in}}\\mathbb{E}[w_{ij}^2x_j^2]%\n",
    "=\\sum_{j=1}^{n_\\text{in}}\\mathbb{E}[w_{ij}^2]\\mathbb{E}[x_j^2]%\n",
    "=n_\\text{in}\\sigma^2\\gamma^2%\n",
    "\\end{align*}\n",
    "* De este resultado se deduce que $n_{\\text{in}}\\sigma^2=1$ evita que la varianza explote\n",
    "* Siguiendo un razonamiento análogo, se deduce que $n_{\\text{out}}\\sigma^2=1$ evita que la varianza de los gradientes explote en backward\n",
    "* La $\\sigma^2$ escogida pretende garantizar ambas condiciones\n",
    "\n",
    "**Inicialización LeCun:** $\\;\\sigma^2=\\dfrac{1}{n_{\\text{in}}}$, esto es, Glorot con $n_{\\text{in}}=n_{\\text{out}}$\n",
    "\n",
    "**Inicialización He:** $\\;\\sigma^2=\\dfrac{2}{n_{\\text{in}}}$\n",
    "\n",
    "**Inicialización con una uniforme:** $\\;$ si $w_{ij}\\sim\\operatorname{Unif}(-a,a),\\,$ $\\mathbb{E}[o_i]=0,\\,$ $\\,\\mathbb{V}[o_i]=\\frac{a^2}{3}\\,$ y tomamos $a=\\sqrt{\\frac{6}{n_{\\text{in}}+n_{\\text{out}}}}$\n",
    "\n",
    "**Recomendaciones:** $\\;$ Glorot para lineal, tanh, logística y softmax; He para ReLU i variantes; LeCun para SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializaciones dirigidas por los datos\n",
    "\n",
    "**Inicialización layer-sequential unit-variance (LSUV):**\n",
    "\n",
    "* Inicializamos los pesos de cada capa con matrices ortonormales; p.e., muestreando $\\boldsymbol{w}\\in\\mathcal{N}(\\boldsymbol{0},\\mathbf{I})$, reformateando $\\boldsymbol{w}$ como una matriz $\\boldsymbol{w}$ y hallando una base ortonormal con QR o SVD\n",
    "* Para cada capa $l$, calculamos la varianza de las activaciones en un minibatch, $v_l$, y re-escalamos $\\boldsymbol{w}_l=\\boldsymbol{w}_l/\\sqrt{v_l}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación como espacio funcional\n",
    "\n",
    "**Neural network Gaussian process:** $\\;$ dado que distribuciones diferentes sobre los parámetros corresponden a distribuciones diferentes sobre funciones, la inicialización del modelo puede verse como un muestreo de un punto sobre el prior; en el límite de redes infinitamente anchas, el prior puede derivarse analíticamente"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
