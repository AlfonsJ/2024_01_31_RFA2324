{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión\n",
    "\n",
    "En los problemas de **regresión,** la salida es un real $y\\in\\mathbb{R}$ en lugar de una etiqueta de clase $y\\in\\{1,\\dotsc,C\\}$, como ocurre en clasificación. La función de pérdida usual en regresión es la **pérdida cuadrática** o **L2:**\n",
    "$$\\ell_2(y,\\hat{y})=(y-\\hat{y})^2$$\n",
    "La diferencia entre observación y predicción, $y-\\hat{y}$, recibe el nombre de **residuo.** Llamamos **error cuadrático medio (MSE, mean squared error)** al riesgo empírico con pérdida cuadrática:\n",
    "$$\\operatorname{MSE}(\\boldsymbol{\\theta})=\\frac{1}{N}\\sum_n (y_n-f(\\boldsymbol{x}_n; \\boldsymbol{\\theta}))^2$$\n",
    "El modelo probabilístico condicional que suele usarse para capturar la incertidumbre subyacente es la **distribución normal o Gaussiana condicional (a posteriori):**\n",
    "$$p(y\\mid \\boldsymbol{x}; \\boldsymbol{\\theta})%\n",
    "=\\mathcal{N}(y\\mid \\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[{-\\frac{1}{2\\sigma^2}(y-\\mu)^2}\\right]%\n",
    "\\qquad\\text{con}\\qquad%\n",
    "\\mu=f(\\boldsymbol{x}; \\boldsymbol{\\theta})$$\n",
    "Nótese que la función predictora se limita a predecir la media (a posteriori); por simplicidad en este contexto, la varianza $\\sigma^2$ se asume fija. Por otro lado, se puede comprobar que la neg-log-verosimilitud es proporcional al MSE:\n",
    "$$\\begin{align*}\\operatorname{NLL}(\\boldsymbol{\\theta})%\n",
    "&=-\\frac{1}{N}\\sum_n\\log\\left[\\left(2\\pi\\sigma^2\\right)^{-\\frac{1}{2}}%\n",
    "\\exp\\left(-\\frac{1}{2\\sigma^2}(y_n-f(\\boldsymbol{x}_n; \\boldsymbol{\\theta}))^2\\right)\\right]\\\\%\n",
    "&=\\frac{1}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}\\frac{1}{N}\\sum_n (y_n-f(\\boldsymbol{x}_n; \\boldsymbol{\\theta}))^2\\\\%\n",
    "&=\\operatorname{const}+\\frac{1}{2\\sigma^2}\\operatorname{MSE}(\\boldsymbol{\\theta})\n",
    "\\end{align*}$$\n",
    "Así pues, el MLE de $\\boldsymbol{\\theta}$ también minimiza el MSE. Claramente, la difilcultad de un problema de regresión depende en gran medida de la dimensión $D$ de la entrada. Si la entrada es unidimensional, $x\\in\\mathbb{R}$, hablamos de **regresión simple**; si no, si $\\boldsymbol{x}\\in\\mathbb{R}^D$, $D>1$, hablamos de **regresión múltiple.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
