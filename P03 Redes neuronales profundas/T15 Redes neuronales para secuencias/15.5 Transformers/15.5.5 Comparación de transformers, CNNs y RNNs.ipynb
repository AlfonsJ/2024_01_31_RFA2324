{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.5.5 Comparación de transformers, CNNs y RNNs\n",
    "\n",
    "**Arquitecturas para transformar una secuencia $\\boldsymbol{x}_{1:n}$ en otra $\\boldsymbol{y}_{1:n}$:**\n",
    "<div align=center><img src=\"Figure_15.27.png\" width=600></td></tr></div>\n",
    "\n",
    "**Comparación:** $\\;n$ es la longitud de la secuencia, $d$ la dimensión de las características de entrada y $k$ la talla del kernel convolucional\n",
    "\n",
    "<center>\n",
    "\n",
    "|Capa|Complejidad|Operaciones secuenciales|Max. longitud de camino|\n",
    "|:-|:-:|:-:|:-:|\n",
    "|Auto-atención|$O(n^2d)$|$O(1)$|$O(1)$|\n",
    "|Recurrente|$O(nd^2)$|$O(n)$|$O(n)$|\n",
    "|Convolucional|$O(knd^2)$|$O(1)$|$O(\\log_k n)$|\n",
    "\n",
    "</center>\n",
    "\n",
    "* **CNN 1d:** $O(knd^2)$ para calcular la salida en paralelo, aunque se necesita una pila de $n/k$ capas, o $\\log_k n$ con dilataciones, para comunicar todos los pares (figura: $x_1$ y $x_5$ conectan en la capa 2)\n",
    "* **RNN:** $O(nd^2)$ por producto matriz-vector en cada paso secuencial\n",
    "* **Auto-atención:** $O(1)$ máxima longitud de camino ya que cada salida se conecta directamente a cada entrada; el coste $O(n^2d)$ es asumible para secuencias cortas, pero no para largas"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
