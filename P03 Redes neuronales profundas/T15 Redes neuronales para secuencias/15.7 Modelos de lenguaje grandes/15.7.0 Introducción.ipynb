{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.7.0 Introducción\n",
    "\n",
    "**Contextual word embeddings:** $\\;$ las RNNs y transformers auto-regresivos (decoder) son modelos de lenguaje que generan secuencias de la forma $\\,p(x_1,\\dotsc,x_T)=\\prod_tp(x_t\\mid\\boldsymbol{x}_{1:t-1})$; en lugar de un vector one-hot $\\boldsymbol{x}_t$ o un embedding aprendido, usamos los estados latentes de dichos modelos a modo de embedding contextual para tareas seq2seq o clasificación de texto\n",
    "\n",
    "**Transfer learning:** $\\;$ en **pre-training,** pre-entrenamos un modelo de lenguaje sin supervisión, con un corpus de texto grande; luego, en **fine-tuning,** ajustamos el modelo pre-entrenado a la tarea con un conjunto pequeño y supervisado\n",
    "\n",
    "**Modelos causales y no-causales:** $\\;$ los causales generan texto; los no-causales no generan texto, sino que se orientan a la representación de frases con $\\boldsymbol{h}_t$ dependiente de pasado $\\boldsymbol{y}_{1:t-1}$, presente $\\boldsymbol{y}_t$ y futuro $\\boldsymbol{y}_{t+1:T}$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
