{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4.0 Introducción\n",
    "\n",
    "**Optimización estocástica:** $\\;$ minimizamos el valor esperado del objetivo con respecto a cierta variable aleatoria $\\boldsymbol{z}$ añadida\n",
    "$$\\mathcal{L}(\\boldsymbol{\\theta})=\\mathbb{E}_{q(\\boldsymbol{z})}[\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{z})]$$\n",
    "\n",
    "**Stochastic gradient descent (SGD):** $\\;$ en la iteración $t$ observamos $\\;\\mathcal{L}_t(\\boldsymbol{\\theta})=\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{z}_t)\\;$ con $\\,\\boldsymbol{z}_t\\sim q$\n",
    "$$\\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t-\\eta_t\\,\\nabla\\mathcal{L}(\\boldsymbol{\\theta}_t,\\boldsymbol{z}_t)=\\boldsymbol{\\theta}_t-\\eta_t\\,\\boldsymbol{g}_t$$\n",
    "donde $\\nabla\\mathcal{L}(\\boldsymbol{\\theta}_t,\\boldsymbol{z}_t)$ es un estimador insesgado del gradiente de $\\mathcal{L}$;\n",
    "por ejemplo, $\\boldsymbol{g}_t=\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_t(\\boldsymbol{\\theta}_t)\\,$ si $\\,q(\\boldsymbol{z})$ no depende de $\\boldsymbol{\\theta}$\n",
    "\n",
    "**Convergencia de SGD:** $\\;$ si $\\boldsymbol{g}_t$ es insesgado, converge a un punto estacionario"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
