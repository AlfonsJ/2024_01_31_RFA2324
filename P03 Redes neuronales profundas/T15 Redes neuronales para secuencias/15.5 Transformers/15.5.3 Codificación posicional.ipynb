{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.5.3 Codificación posicional\n",
    "\n",
    "**Positional encoding:** $\\;$ dado que auto-atención es invariante a permutaciones, esto es, ignora el orden de ocurrencia de las palabras, se suele combinar el embedding de cada palabra con otro que represente su posición (aunque no está claro que convenga hacerlo)\n",
    "\n",
    "**Representación mediante un entero:** $\\;$ para una secuencia de longitud $n$\n",
    "* $p_i=i\\,$ dificulta el entrenamiento de la red con longitudes grandes\n",
    "* $p_i=i/n\\,$ no funciona bien con secuencias de longitud variable\n",
    "* $\\boldsymbol{p}_i\\in\\{0,1\\}^d\\,$ igual a la representación binaria de $i$ (con $d$ bits) dificulta la predicción de $i+k$ ($k$ const.)\n",
    "\n",
    "**Representación mediante sinusoides:** $\\;$ para una secuencia de longitud $n\\leq C$ (p.e. $C=10000$), con $d$ reales\n",
    "$$p_{i,2j}=\\sin\\left(\\frac{i}{C^{2j/d}}\\right)%\n",
    "\\qquad\\text{y}\\qquad%\n",
    "p_{i,2j+1}=\\cos\\left(\\frac{i}{C^{2j/d}}\\right)%\n",
    "\\qquad(j=0,1,\\dotsc)$$\n",
    "\n",
    "$$\\boldsymbol{p}_i=\\left[\\sin\\left(\\frac{i}{C^{0/4}}\\right),\\cos\\left(\\frac{i}{C^{0/4}}\\right),\\sin\\left(\\frac{i}{C^{2/4}}\\right),\\cos\\left(\\frac{i}{C^{2/4}}\\right)\\right]\\qquad(d=4)$$\n",
    "\n",
    "*Ejemplo:* $\\;$ codificaciones posicionales para $\\,n=60\\,$ y $\\,d=32\\,$\n",
    "<div><table border-collapse: collapse; align=center><tr>\n",
    "<td style=\"border: none; padding:0; margin:0;\"><img src=\"Figure_15.25_A.png\" width=\"300\"/></td>\n",
    "<td style=\"border: none; padding:0; margin:0;\"><img src=\"Figure_15.25_B.png\" width=\"650\"/></td>\n",
    "</tr></table></div>\n",
    "\n",
    "**Ventajas de la representación sinusoide:**\n",
    "* Calculable para entradas de longitud arbitraria hasta $n\\leq C$\n",
    "* $\\boldsymbol{p}_{i+\\phi}$ puede predecirse como transformación lineal de $\\boldsymbol{p}_i$\n",
    "$$\\begin{pmatrix}\\sin(\\omega_j(i+\\phi))\\\\\\cos(\\omega_j(i+\\phi))\\end{pmatrix}%\n",
    "=\\begin{pmatrix}\n",
    "\\sin(\\omega_ji)\\cos(\\omega_j\\phi)+\\cos(\\omega_ji)\\sin(\\omega_j\\phi)\\\\%\n",
    "\\cos(\\omega_ji)\\cos(\\omega_j\\phi)-\\sin(\\omega_ji)\\sin(\\omega_j\\phi)%\n",
    "\\end{pmatrix}%\n",
    "=\\begin{pmatrix}\\cos(\\omega_j\\phi)&\\sin(\\omega_j\\phi)\\\\-\\sin(\\omega_j\\phi)&\\cos(\\omega_j\\phi)\\end{pmatrix}%\n",
    "\\begin{pmatrix}\\sin(\\omega_ji)\\\\\\cos(\\omega_ji)\\end{pmatrix}$$\n",
    "* Si $\\phi$ es pequeño, $\\;\\boldsymbol{p}_{i+\\phi}\\approx \\boldsymbol{p}_i$\n",
    "\n",
    "**Combinación del embedding posicional $\\mathbf{P}\\in\\mathbb{R}^{n\\times d}$ con el de palabras, $\\mathbf{X}$:** $\\;$ podria hacerse por concatenación, pero se prefiere la suma\n",
    "$$\\operatorname{POS}(\\operatorname{Embed}(\\mathbf{X}))=\\mathbf{X}+\\mathbf{P}$$\n",
    "* En realidad, no está claro que convenga usar codificaciones posicionales (ver [Kazemnejad23](https://arxiv.org/abs/2305.19466))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
